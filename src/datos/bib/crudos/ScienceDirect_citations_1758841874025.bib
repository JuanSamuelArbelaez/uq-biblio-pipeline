@article{THAKKAR2024110452,
title = {Advances in materials and machine learning techniques for energy storage devices: A comprehensive review},
journal = {Journal of Energy Storage},
volume = {81},
pages = {110452},
year = {2024},
issn = {2352-152X},
doi = {https://doi.org/10.1016/j.est.2024.110452},
url = {https://www.sciencedirect.com/science/article/pii/S2352152X24000379},
author = {Prit Thakkar and Sachi Khatri and Drashti Dobariya and Darpan Patel and Bishwajit Dey and Alok Kumar Singh},
keywords = {Material science, Machine learning, Battery, Supercapacitor, Energy technology},
abstract = {The increasing global need for energy supply in modern society has created a pressing need to explore new materials for renewable energy technologies. However, conventional trial and error methods in materials science are often tedious as well as costly, making it challenging to meet the growing demands. In recent years, machine learning (ML) become a prominent research strategy transfigure the discovery of materials. This review offers a concise summary of the elementary ML procedures and widely used algorithms in the field of materials science. It particularly emphasizes the latest advancements in utilizing ML for predicting material properties and developing materials for energy-related fields like Li-Ion batteries, Super-Capacitors, and Hybrid Systems. Furthermore, the review discusses the contributions of ML to experimental research. This review intents to serve as a guiding resource for future developments of ML in materials science.}
}
@article{NAWN2024128998,
title = {The distal-proximal relationships among the human moonlighting proteins: Evolutionary hotspots and Darwinian checkpoints},
journal = {International Journal of Biological Macromolecules},
volume = {259},
pages = {128998},
year = {2024},
issn = {0141-8130},
doi = {https://doi.org/10.1016/j.ijbiomac.2023.128998},
url = {https://www.sciencedirect.com/science/article/pii/S014181302305897X},
author = {Debaleena Nawn and Sk. Sarif Hassan and Moumita Sil and Ankita Ghosh and Arunava Goswami and Pallab Basu and Guy W. Dayhoff and Kenneth Lundstrom and Vladimir N. Uversky},
keywords = {Moonlighting proteins, Phylogeny, Distal-proximal, Promiscuous proteins, Signature-features},
abstract = {Moonlighting proteins, known for their ability to perform multiple, often unrelated functions within a single polypeptide chain, challenge the traditional “one gene, one protein, one function” paradigm. As organisms evolved, their genomes remained relatively stable in size, but the introduction of post-translational modifications and sub-strategies like protein promiscuity and intrinsic disorder enabled multifunctionality. Enzymes, in particular, exemplify this phenomenon, engaging in unrelated processes alongside their primary catalytic roles. This study employs a systematic, quantitative informatics approach to shed light on human moonlighting protein sequences. Phylogenetic analyses of human moonlighting proteins are presented, elucidating the distal-proximal relationships among these proteins based on sequence-derived quantitative features. The findings unveil the captivating world of human moonlighting proteins, urging further investigations in the emerging field of moonlighting proteomics, with the potential for significant contributions to our understanding of multifunctional proteins and their roles in diverse cellular processes and diseases.}
}
@article{NUTARELLI2025107011,
title = {Predicting the technological complexity of global cities based on unsupervised and supervised machine learning methods},
journal = {Journal of Economic Behavior & Organization},
volume = {234},
pages = {107011},
year = {2025},
issn = {0167-2681},
doi = {https://doi.org/10.1016/j.jebo.2025.107011},
url = {https://www.sciencedirect.com/science/article/pii/S0167268125001301},
author = {Federico Nutarelli and Samuel Edet and Giorgio Gnecco and Massimo Riccaboni},
keywords = {Innovation, Urban studies, Technological change, Artificial intelligence, Global cities},
abstract = {Analyzing and predicting innovation in global cities, i.e. cities with a high degree of economic integration into the world economy, can help identify emerging technologies and inform investment decisions that facilitate talent attraction and urban planning. In this context, the contribution of this paper is to analyze the technological complexity of global cities. We show how the combination of state-of-the-art network community detection and supervised machine learning can support local innovation and development policies by predicting the future competitiveness of global cities based on an up-to-date patent dataset. Network community detection with the Poisson stochastic block model is used as an unsupervised pre-processing step to find cities with similar innovation profiles and create homogeneous training sets that improve predictive power, interpretability and computational efficiency in a subsequent supervised learning task. The paper then compares the use of different supervised machine learning methods to predict the future competitiveness of global cities. Tree-based methods turn out to achieve better prediction performance than other supervised machine learning methods on various metrics based on the ground truth derived from historical patent production. The analytical method used in this paper can help policy makers identify technology sectors where global cities could focus their future investments and provide information on the temporal evolution of geographical patterns related to innovation.}
}
@article{VERAAMARO2025107821,
title = {Towards accessible website design through artificial intelligence: A systematic literature review},
journal = {Information and Software Technology},
volume = {186},
pages = {107821},
year = {2025},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2025.107821},
url = {https://www.sciencedirect.com/science/article/pii/S0950584925001600},
author = {Guillermo Vera-Amaro and José Rafael Rojano-Cáceres},
keywords = {Web accessibility, Systematic literature review, Artificial intelligence, Wcag, Machine learning, Large language models},
abstract = {Context:
Web accessibility ensures that individuals with disabilities can access, navigate, and interact with online content effectively. Despite the availability of the Web Content Accessibility Guidelines (WCAG), significant barriers persist, largely due to the complexity of their implementation. Artificial intelligence (AI), particularly machine learning models, has emerged as a promising avenue to address these challenges, offering solutions for evaluation, correction, and content generation.
Objective:
This study aims to systematically review the intersection of web accessibility and AI by evaluating how AI-based methods enhance compliance with accessibility standards over the period 2019–2025, assessing their efficacy and alignment with WCAG principles.
Methods:
A systematic literature review (SLR) was conducted in three phases: planning, execution, and reporting. Research questions were formulated guiding the selection of search terms and strategies. A systematic search process was implemented, complemented by a snowballing technique to ensure comprehensive coverage of relevant studies. The quality of selected studies was rigorously assessed using predefined criteria, and data extraction was carried out following established best practices. The analysis combined narrative synthesis for qualitative insights and bibliometric techniques for quantitative evaluation.
Results:
From 277 studies, 31 were identified as relevant, highlighting AI’s primary contributions to generating alternative text for images, automating compliance assessments, providing correction suggestions, and designing alternative interfaces to enhance accessibility. Advances in large language models (LLMs) further demonstrate their potential for facilitating the creation of accessible content.
Conclusions:
AI presents significant potential to improve web accessibility by streamlining evaluation processes and supporting the creation of accessible content. However, further research is needed to address limitations such as inconsistent compliance and the lack of focus on non-visual disabilities. These findings underline the importance of leveraging AI to facilitate inclusive web design practices while ensuring adherence to accessibility standards.}
}
@article{RUNGRUANGJIT2024100523,
title = {The power of human-like virtual-influencer-generated content: Impact on consumers’ willingness to follow and purchase intentions},
journal = {Computers in Human Behavior Reports},
volume = {16},
pages = {100523},
year = {2024},
issn = {2451-9588},
doi = {https://doi.org/10.1016/j.chbr.2024.100523},
url = {https://www.sciencedirect.com/science/article/pii/S2451958824001568},
author = {Warinrampai Rungruangjit and Kulachet Mongkol and Intaka Piriyakul and Kitti Charoenpornpanichkul},
keywords = {Artificial intelligence, Emotional content, Generated content, Uses and gratifications theory, Virtual influencer, Willingness to follow},
abstract = {The swift progress in machine learning algorithms, artificial intelligence, and interactive immersive media technologies has led to the introduction of computer-generated imagery on Instagram. This feature, so-called “human-like virtual influencers (VIs)", has revolutionized the way people interact with technology. Using a combination of cutting-edge AI technologies, in a novel application of computer vision algorithms, and large language models to extract the content posted by two popular human-like VIs on Instagram, the present study is the first to categorize and classify types of human-like virtual-influencer-generated content. Quantitative methods, such as partial least squares structural equation modeling (PLS-SEM), were used to examine the impact of human-like virtual-influencer-generated content on consumers' willingness to follow as well as purchase intentions. The information was gathered from 650 Thai customers. The findings showed that consumers' willingness to follow and purchase intentions were significantly influenced by the positive effects of emotional appeal content, which includes relational, entertaining, positive emotion, and negative emotion content. These effects outweighed those of rational appeal content, such as informative and remunerative content, as well as authenticity appeal content. Meanwhile, disclosing sponsored content had no effect on consumers' willingness to follow. The theoretical underpinnings of uses and gratifications (U&G) theory, parasocial relationships and Richins' hierarchical model of emotions are confirmed and expanded upon in this work, and the suggested inclusive approach also significantly advances the expanding corpus of research on VIs. Our research also provides a contribution to the recent literature on human-like VI marketing.}
}
@article{GIAMATTEI2025107599,
title = {Causal reasoning in Software Quality Assurance: A systematic review},
journal = {Information and Software Technology},
volume = {178},
pages = {107599},
year = {2025},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2024.107599},
url = {https://www.sciencedirect.com/science/article/pii/S0950584924002040},
author = {Luca Giamattei and Antonio Guerriero and Roberto Pietrantuono and Stefano Russo},
keywords = {Causal reasoning, Causal discovery, Causal inference, Software quality},
abstract = {Context:
Software Quality Assurance (SQA) is a fundamental part of software engineering to ensure stakeholders that software products work as expected after release in operation. Machine Learning (ML) has proven to be able to boost SQA activities and contribute to the development of quality software systems. In this context, Causal Reasoning is gaining increasing interest as a methodology to go beyond a purely data-driven approach by exploiting the use of causality for more effective SQA strategies.
Objective:
Provide a broad and detailed overview of the use of causal reasoning for SQA activities, in order to support researchers to access this research field, identifying room for application, main challenges and research opportunities.
Methods:
A systematic review of the scientific literature on causal reasoning for SQA. The study has found, classified, and analyzed 86 articles, according to established guidelines for software engineering secondary studies.
Results:
Results highlight the primary areas within SQA where causal reasoning has been applied, the predominant methodologies used, and the level of maturity of the proposed solutions. Fault localization is the activity where causal reasoning is more exploited, especially in the web services/microservices domain, but other tasks like testing are rapidly gaining popularity. Both causal inference and causal discovery are exploited, with the Pearl’s graphical formulation of causality being preferred, likely due to its intuitiveness. Tools to favor their application are appearing at a fast pace — most of them after 2021.
Conclusions:
The findings show that causal reasoning is a valuable means for SQA tasks with respect to multiple quality attributes, especially during V&V, evolution and maintenance to ensure reliability, while it is not yet fully exploited for phases like requirements engineering and design. We give a picture of the current landscape, pointing out exciting possibilities for future research.}
}
@article{MUHAMMAD2025216602,
title = {Hemicyanine-based fluorescent probes: Advancements in biomedical sensing and activity-based detection},
journal = {Coordination Chemistry Reviews},
volume = {534},
pages = {216602},
year = {2025},
issn = {0010-8545},
doi = {https://doi.org/10.1016/j.ccr.2025.216602},
url = {https://www.sciencedirect.com/science/article/pii/S0010854525001729},
author = {Sibtain Muhammad and Haroon Ahmad and Yuqian Yan and Xin Chen and Saz Muhammad and Madappa C. Maridevaru and Shubham Roy and Zun Wang and Yinghe Zhang and Bing Guo},
keywords = {Hemicyanine dyes, NIR fluorescence probes, Biomarker detection, Non-intrusive, Imaging, Polymethine dyes},
abstract = {Near-infrared (NIR) fluorescent probes have revolutionized in vivo imaging by enabling the real-time detection of biomarkers for disease diagnosis and drug screening. Among these, donor-π-acceptor (D-π-A) architecture of hemicyanine dyes have gained attention for their high fluorescence quantum yield, ease in structural versatility, and good capability to construct activatable probes. Hemicyanine-based NIR activatable probes (HNAPs) leverage intramolecular charge transfer (ICT) mechanisms for precise, biomarker-responsive fluorescence activation, making them invaluable tools in preclinical and clinical research. This review highlights the principles of molecular design and applications of HNAPs for detecting key biomarkers associated with several diseases, including as skin conditions, digestive problems, cancer, inflammation, and acute organ failure. These probes enable real-time imaging with high specificity, addressing critical clinical challenges in early diagnosis and monitoring disease progression. Moreover, the translational potential of HNAPs lies in their capacity for non-invasive imaging and targeted biomarker detection, paving the way for innovations in imaging-guided diagnosis and treatment strategies. The versatility of hemicyanine scaffolds and their ability to be tailored for diverse applications underscore their unique value in bioimaging. This progress emphasizes the transformative potential of HNAPs in advancing precision diagnostics and improving clinical outcomes. We hope this review would stimulate interest among wide researchers and expedite the clinic translation for HNAPs.}
}
@article{ROSSEEL2025109992,
title = {A state-of-the-art review on acoustic preservation of historical worship spaces through auralization},
journal = {Signal Processing},
volume = {234},
pages = {109992},
year = {2025},
issn = {0165-1684},
doi = {https://doi.org/10.1016/j.sigpro.2025.109992},
url = {https://www.sciencedirect.com/science/article/pii/S0165168425001069},
author = {Hannes Rosseel and Toon {van Waterschoot}},
keywords = {Acoustic preservation, Historical worship spaces, Room acoustics, Auralization},
abstract = {Historical Worship Spaces (HWS) are significant architectural landmarks which hold both cultural and spiritual value. The acoustic properties of these spaces play a crucial role in historical and contemporary religious liturgies, rituals, and ceremonies, as well as in the performance of sacred music. However, the original acoustic characteristics of these spaces are often at risk due to repurposing, renovations, natural disasters, or deterioration over time. This paper presents a comprehensive review of the current state of research on the acquisition, analysis, and synthesis of acoustics, with a focus on HWS. An example case study of the Nassau chapel in Brussels, Belgium, is presented to demonstrate the application of these techniques for the preservation and auralization of historical worship space acoustics. The paper concludes with a discussion of the challenges and opportunities in the field, and outlines future research directions.}
}
@article{LI20243825,
title = {Survey and Prospect for Applying Knowledge Graph in Enterprise Risk Management},
journal = {Computers, Materials and Continua},
volume = {78},
number = {3},
pages = {3825-3865},
year = {2024},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2024.046851},
url = {https://www.sciencedirect.com/science/article/pii/S1546221824003618},
author = {Pengjun Li and Qixin Zhao and Yingmin Liu and Chao Zhong and Jinlong Wang and Zhihan Lyu},
keywords = {Knowledge graph, enterprise risk, risk identification, risk management, review},
abstract = {Enterprise risk management holds significant importance in fostering sustainable growth of businesses and in serving as a critical element for regulatory bodies to uphold market order. Amidst the challenges posed by intricate and unpredictable risk factors, knowledge graph technology is effectively driving risk management, leveraging its ability to associate and infer knowledge from diverse sources. This review aims to comprehensively summarize the construction techniques of enterprise risk knowledge graphs and their prominent applications across various business scenarios. Firstly, employing bibliometric methods, the aim is to uncover the developmental trends and current research hotspots within the domain of enterprise risk knowledge graphs. In the succeeding section, systematically delineate the technical methods for knowledge extraction and fusion in the standardized construction process of enterprise risk knowledge graphs. Objectively comparing and summarizing the strengths and weaknesses of each method, we provide recommendations for addressing the existing challenges in the construction process. Subsequently, categorizing the applied research of enterprise risk knowledge graphs based on research hotspots and risk category standards, and furnishing a detailed exposition on the applicability of technical routes and methods. Finally, the future research directions that still need to be explored in enterprise risk knowledge graphs were discussed, and relevant improvement suggestions were proposed. Practitioners and researchers can gain insights into the construction of technical theories and practical guidance of enterprise risk knowledge graphs based on this foundation.}
}
@article{WANG2025116100,
title = {Progress in beamforming acoustic imaging based on phased microphone arrays: Algorithms and applications},
journal = {Measurement},
volume = {242},
pages = {116100},
year = {2025},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2024.116100},
url = {https://www.sciencedirect.com/science/article/pii/S0263224124019857},
author = {Yong Wang and Zhi Deng and Jiaxi Zhao and Victor Feliksovich Kopiev and Donglai Gao and Wen-Li Chen},
keywords = {Phased microphone arrays, Acoustic imaging, Beamforming, Source location, Time-delay estimation},
abstract = {Beamforming acoustic imaging technology, utilizing phased microphone arrays, enables precise sound source localization and finds widespread application in aerodynamic wind tunnel testing, acoustic signal recognition, and mechanical fault diagnosis. This paper presents a comprehensive review of beamforming evolution, detailing its mathematical foundations and diverse applications in acoustic imaging. Various beamforming methodologies are critically analyzed using wind tunnel test data, and an overview of correction methods for external interferences and array optimization approaches is provided. Through this examination, the strengths and limitations of each method are highlighted, offering insights for future research. Additionally, potential future enhancements, including paradigm-shift approaches to advance beamforming capabilities, are explored, suggesting directions for further innovation. This review aims to establish a foundation for newcomers to the field, stimulate academic discussion, and drive ongoing research in acoustic imaging. By elucidating beamforming complexities, correction methods, and optimization techniques, this study seeks to enhance collective knowledge and support continued advancements in this technology.}
}
@article{HAN2024101007,
title = {Recent progress of efficient low-boom design and optimization methods},
journal = {Progress in Aerospace Sciences},
volume = {146},
pages = {101007},
year = {2024},
issn = {0376-0421},
doi = {https://doi.org/10.1016/j.paerosci.2024.101007},
url = {https://www.sciencedirect.com/science/article/pii/S0376042124000332},
author = {Zhonghua Han and Jianling Qiao and Liwen Zhang and Qing Chen and Han Yang and Yulin Ding and Keshi Zhang and Wenping Song and Bifeng Song},
keywords = {Sonic boom, Supersonic transport, Low-boom design, Surrogate-based optimization, Adjoint method},
abstract = {Reducing the sonic boom to a community-acceptable level is a fundamental challenge in the configuration design of the next-generation supersonic transport aircraft. This paper conducts a survey of recent progress in developing efficient low-boom design and optimization methods, and provides a perspective on the state-of-the-art and future directions. First, the low- and high-fidelity sonic boom prediction methods used in metric of low-boom design are briefly introduced. Second, efficient low-boom inverse design methods are reviewed, such as the classic Jones–Seebass–George–Darden (JSGD) method (and its variants), the high-fidelity near-field-overpressure-based method, and the mixed-fidelity method. Third, direct numerical optimization methods for low-boom designs, including the gradient-, surrogate-, and deep-learning-based optimization methods, are reviewed. Fourth, the applications of low-boom design and optimization methods to representative low-boom configurations are discussed, and the challenging demands for commercially viable supersonic transports are presented. In addition to providing a comprehensive summary of the existing research, the practicality and effectiveness of the developed methods are assessed. Finally, key challenges are identified, and further research directions such as full-carpet-low-boom-driven multidisciplinary design optimization considering mission requirements are recommended.}
}
@article{CHAKRABORTY2024100677,
title = {Role of human physiology and facial biomechanics towards building robust deepfake detectors: A comprehensive survey and analysis},
journal = {Computer Science Review},
volume = {54},
pages = {100677},
year = {2024},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2024.100677},
url = {https://www.sciencedirect.com/science/article/pii/S1574013724000613},
author = {Rajat Chakraborty and Ruchira Naskar},
keywords = {Deepfake, Facial biomechanics, Generative AI, Physiological signal, rPPG, Social engineering},
abstract = {AI based multimedia content generation, already having achieved hyper-realism, deeply influences human perception and trust. Since emerging around late 2017, deepfake technology has rapidly gained popularity due to its diverse applications, raising significant concerns regarding its malicious and unethical use. Although many deepfake detectors have been developed by forensic researchers in recent years, there is an urgent need for robust detectors that can overcome demographic, social, and cultural barriers in identifying deepfakes. To identify a human as a human, to distinguish a person from a synthetic entity, the literature faces compelling necessity to introduce deepfake detectors that can withstand all forms of demographic and social biases. (Multiple researches have been conducted in recent times to prove the existence of social and demographic biases in synthetic media detectors.) In this article, we examine human physiological signals as the foundation for robust deepfake detectors, and present a survey of recent developments in deepfake detection research that relies on human physiological signals and facial biomechanics. We perform in-depth analysis of the techniques to understand the contribution of human physiology in deepfake detection. Hence, we comprehend how human physiology based deepfake detectors fare by exploiting the inherent robustness of physiological signals, in contrast to other existing detectors.}
}
@incollection{MOURTZIS202413,
title = {2 - Industry 4.0 and smart manufacturing},
editor = {Dimitris Mourtzis},
booktitle = {Manufacturing from Industry 4.0 to Industry 5.0},
publisher = {Elsevier},
pages = {13-61},
year = {2024},
isbn = {978-0-443-13924-6},
doi = {https://doi.org/10.1016/B978-0-443-13924-6.00002-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780443139246000028},
author = {Dimitris Mourtzis},
keywords = {Computer science, operations management, information systems, computing, human-centered computing, systems engineering, software engineering, human–computer interaction, robotics, technology management, knowledge management, specific industry, data science, sustainability engineering, applied computing, sustainable development, computing methodology},
abstract = {Industry 4.0 and smart manufacturing are the latest technological advancements that have revolutionized the manufacturing industry. Industry 4.0 refers to the Fourth Industrial Revolution, which involves the integration of digital technologies such as the Internet of Things, Big Data, and Artificial Intelligence into the manufacturing process. Next, smart manufacturing is the application of Industry 4.0 technologies to improve the efficiency, productivity, and flexibility of manufacturing systems, processes, and services. Industry 4.0 and smart manufacturing have several benefits, such as improved production efficiency, reduced costs, enhanced product quality, and increased agility in responding to market changes. However, implementing these technologies requires significant investment in infrastructure, skilled personnel, and technology upgrades. To address these challenges and fully leverage the potential of Industry 4.0 and smart manufacturing, manufacturers need to develop a clear strategy, which includes identifying the areas of the business that can benefit the most from the technologies. In addition, manufacturers need to train their workforce to use and maintain the new technologies, while also addressing concerns about job displacement. In summary, Industry 4.0 and smart manufacturing offer significant potential for manufacturers to improve their operations, reduce costs, and stay competitive. However, it requires careful planning and investment to fully realize the benefits.}
}
@article{BERNARDES2025104570,
title = {Semantic meaning means a lot: Exploring the role of semantics in the development of a Big Five taxonomy},
journal = {Journal of Research in Personality},
volume = {115},
pages = {104570},
year = {2025},
issn = {0092-6566},
doi = {https://doi.org/10.1016/j.jrp.2024.104570},
url = {https://www.sciencedirect.com/science/article/pii/S0092656624001181},
author = {Gabriel Bernardes and Beatriz Bozza and Marina Motta and Paulo Mattos and Ronald Fischer},
keywords = {Personality taxonomy, Big Five taxonomy, Language and personality, Lexical hypothesis, Lexical approach},
abstract = {Developing a Big Five adjective taxonomy in Brazilian Portuguese, we explored the effects of linguistic properties in our classification processes. The first two studies implement top-down (expert ratings) and bottom-up (self-ratings from a community sample; N = 500) strategies for taxonomy classification and validation. We identified a clear five-factor structure with 171 adjectives supporting the Big Five. Study 3 correlated frequency of use and the semantic dimensions of valence, arousal, and dominance to Big Five measures for each adjective. We found weak effects of frequency, but systematic effects of semantic dimensions with expert ratings and component loadings, that were congruent with differences and overlaps between the five traits. We discuss the potential role of linguistic effects on personality structure and assessment.}
}
@article{ISINGIZWE2024106631,
title = {Enhancing safety training engagement through immersive storytelling: A case study in the residential construction},
journal = {Safety Science},
volume = {179},
pages = {106631},
year = {2024},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2024.106631},
url = {https://www.sciencedirect.com/science/article/pii/S0925753524002212},
author = {Josiane Isingizwe and Ricardo Eiris and Ahmed {Jalil Al-Bayati}},
keywords = {Immersive storytelling, Residential construction, Safety training, Learning engagement, Fall hazards, Eye-tracking},
abstract = {Virtual safety training environments are increasingly utilized to support the development of safety knowledge and increase hazard identification skills in construction. One of the emerging techniques for virtual safety training is immersive storytelling. However, current studies have not explored how the inclusion or exclusion of storytelling within immersive safety training systems produces learning gains. Specifically, this study explores learning through the lens of engagement – behavioral, cognitive, and emotional. The residential construction industry was used as a case study to explore this research gap. Residential workers were assessed through a between-subject experimental design. Two safety training conditions were employed for this evaluation using a between-subject experiment – (1) immersive storytelling; and (2) immersive non-storytelling. The experimental comparison revealed that using immersive storytelling led to increases in behavioral learning and cognitive learning, allowing trainees to effectively identify openings and scaffold fall hazard categories. On the other hand, trainee emotional learning engagement did not change between immersive storytelling and immersive non-storytelling conditions. This study contributes to the existing body of knowledge by providing evidence on how using or not using storytelling can affect learning in the context of safety for fall hazards in residential construction. Practical implications for academicians and industry practitioners for the implementation of storytelling in immersive training systems are provided in the study.}
}
@article{RIQUE2025107586,
title = {Constructing the graphical structure of expert-based Bayesian networks in the context of software engineering: A systematic mapping study},
journal = {Information and Software Technology},
volume = {177},
pages = {107586},
year = {2025},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2024.107586},
url = {https://www.sciencedirect.com/science/article/pii/S0950584924001915},
author = {Thiago Rique and Mirko Perkusich and Kyller Gorgônio and Hyggo Almeida and Angelo Perkusich},
keywords = {Bayesian networks, Bayesian network structure, Expert knowledge, Software engineering, Systematic mapping},
abstract = {Context:
In scenarios where data availability issues hinder the applications of statistical causal modeling in software engineering (SE), Bayesian networks (BNs) have been widely used due to their flexibility in incorporating expert knowledge. However, the general understanding of how the graphical structure, i.e., the directed acyclic graph (DAG), of these models is built from domain experts is still insufficient.
Objective:
This study aims to characterize the SE landscape of constructing the graphical structure of BNs, including their potential for causal modeling.
Method:
We conducted a systematic mapping study employing a hybrid search strategy that combines a database search with parallel backward and forward snowballing.
Results:
Our mapping included a total of 106 studies. Different methods are commonly combined to construct expert-based BN structures. These methods span across data gathering & analysis (e.g., interviews, focus groups, literature research, grounded theory, and statistical analysis) and reasoning mechanisms (e.g., using idioms combined with the adoption of lifecycle models, risk-centric modeling, and other frameworks to guide BN construction). We found a lack of consensus regarding validation procedures, particularly critical when modeling cause–effect relationships from knowledge. Additionally, expert-based BNs are mainly applied at the tactical level to address problems related to software engineering management and software quality. Challenges in creating expert-based structures include validation procedures, experts’ availability, expertise level, and structure complexity handling. Key recommendations involve empirical validation, participatory involvement, and balance between adaptation to organizational constraints and model construction requirements.
Conclusion:
The construction of expert-based BN structures in SE varies in rigor, with some methods being systematic while others appear ad hoc. To enhance BN application, reducing expert knowledge subjectivity, enhancing methodological rigor, and clearly articulating the construction rationale is essential. Addressing these challenges is crucial for improving the reliability of causal inferences drawn from these models, ultimately leading to better-informed decisions in SE practices.}
}
@article{BAIRD20251945,
title = {A call to action: the second Lancet Commission on adolescent health and wellbeing},
journal = {The Lancet},
volume = {405},
number = {10493},
pages = {1945-2022},
year = {2025},
issn = {0140-6736},
doi = {https://doi.org/10.1016/S0140-6736(25)00503-3},
url = {https://www.sciencedirect.com/science/article/pii/S0140673625005033},
author = {Sarah Baird and Shakira Choonara and Peter S Azzopardi and Prerna Banati and Judith Bessant and Olivia Biermann and Anthony Capon and Mariam Claeson and Pamela Y Collins and Nicole {De Wet-Billings} and Surabhi Dogra and Yanhui Dong and Kate L Francis and Luwam T Gebrekristos and Allison K Groves and Simon I Hay and David Imbago-Jácome and Aaron P Jenkins and Caroline W Kabiru and Elissa C Kennedy and Luo Li and Chunling Lu and Jun Ma and Terry McGovern and Augustina Mensa-Kwao and Sanyu A Mojola and Jason M Nagata and Adesola O Olumide and Olayinka Omigbodun and Molly O'Sullivan and Audrey Prost and Jennifer H Requejo and Yusra R Shawar and Jeremy Shiffman and Avi Silverman and Yi Song and Sharlene Swartz and Rita Tamambang and Henrik Urdal and Joseph L Ward and George C Patton and Susan M Sawyer and Alex Ezeh and Russell M Viner}
}
@article{QUINTAIS2025106107,
title = {Generative AI, copyright and the AI Act},
journal = {Computer Law & Security Review},
volume = {56},
pages = {106107},
year = {2025},
issn = {2212-473X},
doi = {https://doi.org/10.1016/j.clsr.2025.106107},
url = {https://www.sciencedirect.com/science/article/pii/S0267364925000020},
author = {João Pedro Quintais},
keywords = {Generative AI, AI Act, AI, Copyright, Text and data mining, Transparency, Remuneration},
abstract = {This paper provides a critical analysis of the Artificial Intelligence (AI) Act's implications for the European Union (EU) copyright acquis, aiming to clarify the complex relationship between AI regulation and copyright law while identifying areas of legal ambiguity and gaps that may influence future policymaking. The discussion begins with an overview of fundamental copyright concerns related to generative AI, focusing on issues that arise during the input, model, and output stages, and how these concerns intersect with the text and data mining (TDM) exceptions under the Copyright in the Digital Single Market Directive (CDSMD). The paper then explores the AI Act's structure and key definitions relevant to copyright law. The core analysis addresses the AI Act's impact on copyright, including the role of TDM in AI model training, the copyright obligations imposed by the Act, requirements for respecting copyright law—particularly TDM opt-outs—and the extraterritorial implications of these provisions. It also examines transparency obligations, compliance mechanisms, and the enforcement framework. The paper further critiques the current regime's inadequacies, particularly concerning the fair remuneration of creators, and evaluates potential improvements such as collective licensing and bargaining. It also assesses legislative reform proposals, such as statutory licensing and AI output levies, and concludes with reflections on future directions for integrating AI governance with copyright protection.}
}
@article{FARHAN2025125963,
title = {RDG-TE: Link reliability-aware DRL-GNN-based traffic engineering in SDN},
journal = {Expert Systems with Applications},
volume = {265},
pages = {125963},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.125963},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424028306},
author = {Muhammad Farhan and Nadir Shah and Lei Wang and Gabriel-Miro Muntean and Houbing Herbert Song},
keywords = {Software defined network, Graph Neural Network, Deep Reinforcement Learning, Traffic engineering},
abstract = {The existing advanced machine learning approaches based on Graph Neural Networks (GNN) for efficient traffic engineering (TE) in Software Defined Networking (SDN) overlook consideration of link reliability values. Link reliability is a very important parameter, directly linked to end-to-end delay for data packet transmission, and can be used to improve the delivery performance. This research article proposes two versions of RDG-TE, a novel model that integrates Deep Reinforcement Learning (DRL) and GNN in order to solve efficiently network TE problems by considering link reliability in both model training and reward computation. The proposed model enables improved performance by more accurately predicting the SDN network behavior in case of link failure. Testing results show that, in comparison to the closest state-of-art approach, our proposed approach reduces the number of disturbed flows by 24.19% and the hop count by 35.38%while also increases the reliable route prediction accuracy by 40.17%.}
}
@article{WU2025127823,
title = {Recent development on online public opinion communication and early warning technologies: Survey},
journal = {Expert Systems with Applications},
volume = {284},
pages = {127823},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.127823},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425014459},
author = {Wei Wu and Yawen Yang and Tianlu Qiao and Haipeng Peng},
keywords = {Online public opinion, Public opinion communication, Public opinion analyzing, Public opinion warning, Artificial intelligence},
abstract = {The rapid development of the Internet and communication technologies has led to the emergence of online public opinion, which plays a crucial role in disseminating information and guiding the public. However, due to its large volume of data and fast-paced changes, automated monitoring of online public opinion is challenging. Currently, advancements in artificial intelligence (AI) technology provides new solutions for detecting and early warning of online public opinion. This paper reviews the literature in the field of online public opinion research over the years. Firstly, it systematically elucidates the formation mechanism and dissemination model of online public opinion. Secondly, it focuses on the analysis of public opinion monitoring technology in the era of artificial intelligence, clarifying the research branches of online public opinion early warning technology. Finally, it identifies the shortcomings in current research on online public opinion and proposes directions for future research.}
}
@article{KHATRI2024100933,
title = {Student well-being in higher education: Scale development and validation with implications for management education},
journal = {The International Journal of Management Education},
volume = {22},
number = {1},
pages = {100933},
year = {2024},
issn = {1472-8117},
doi = {https://doi.org/10.1016/j.ijme.2024.100933},
url = {https://www.sciencedirect.com/science/article/pii/S1472811724000041},
author = {Puja Khatri and Harshleen Kaur Duggal and Weng Marc Lim and Asha Thomas and Atul Shiva},
keywords = {Higher education, Management education, Student well-being, Scale development, Scale validation, SDG3},
abstract = {Student well-being (SWB) encompasses the physical, psychological, and social wellness of students, aspects increasingly at risk in the high-pressure environment of higher education. Marked by intense workloads, unmet expectations, and uncertainties around degree completion and employment, the higher education sector faces a growing challenge in maintaining and enhancing SWB. Current instruments for assessing SWB are limited in scope, failing to capture its multifaceted nature comprehensively. Addressing this gap, our research adopts a rigorous multi-study, multi-method, and multi-sample approach to develop and validate a multi-dimensional scale that effectively captures the nuances of SWB. This process encompassed five methodical stages: scale generation, scale purification, scale refinement, scale validation, and scale generalizability. The resulting SWB scale, encompassing five key dimensions—academic well-being, financial well-being, physical well-being, psychological resilience well-being, and relational well-being—provides a sophisticated tool for measuring and improving SWB in higher education contexts. Crucially, this paper extends beyond the scale development to explore the profound implications of this research for management education. By integrating SWB into management curricula and institutional cultures, this study underscores the potential for higher education to significantly contribute to Sustainable Development Goal 3: Good Health and Well-being. It highlights how nurturing SWB in management education can foster more resilient, empathetic, and socially responsible future leaders, addressing a critical need in contemporary business environments and society at large.}
}
@incollection{IRFAN2025139,
title = {Chapter 8 - Energy efficiency solutions in the digitalized construction sector},
editor = {Ehsan Noroozinejad Farsangi and Mohammad Noori and T.Y Yang and Vasilis Sarhosis and Seyedali Mirjalili and Mirosław J. Skibniewski},
booktitle = {Digital Transformation in the Construction Industry},
publisher = {Woodhead Publishing},
pages = {139-164},
year = {2025},
series = {Woodhead Publishing Series in Civil and Structural Engineering},
isbn = {978-0-443-29861-5},
doi = {https://doi.org/10.1016/B978-0-443-29861-5.00008-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780443298615000081},
author = {Muhammad Irfan and Muhammad Ali Musarat and Wesam Salah Alaloul and Maria Ghufran},
keywords = {Energy efficiency, digitalization, digitized construction sector, energy efficiency solutions, sustainable development.},
abstract = {Energy efficiency refers to the optimized utilization of energy by applying energy-efficient solutions. These solutions encompass various strategies, such as the integration of renewable energy sources with artificial intelligence (AI) and generative AI in construction, advancements in energy storage solutions for buildings, the use of AI for predictive maintenance and energy optimization, the incorporation of AI and building information modeling (BIM), the utilization of robotics, and the implementation of blockchain technology for transparent energy transactions. These energy-efficient solutions can potentially guide stakeholders in transforming the conventional construction industry into a digitized one. Given the significance of energy-efficient solutions in digitized construction, this chapter aims to analyze the current trends and opportunities for implementation in the construction sector. This analysis will consider the various procedures, tools, and technologies available to industry stakeholders.}
}
@article{MUSTAPHA2025103066,
title = {A survey of emerging applications of large language models for problems in mechanics, product design, and manufacturing},
journal = {Advanced Engineering Informatics},
volume = {64},
pages = {103066},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.103066},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624007171},
author = {K.B. Mustapha},
keywords = {Pre-trained language models, Large language models, Generative AI, Generative pre-trained transformer, Mechanical engineering, Engineering design, Manufacturing, Mechanics, Intelligent digital twins, Intelligent maintenance, Creativity},
abstract = {In the span of three years, the application of large language models (LLMs) has accelerated across a multitude of professional sectors. Amid this development, a new collection of studies has manifested around leveraging LLMs for segments of the mechanical engineering (ME) field. Concurrently, it has become clear that general-purpose LLMs faced hurdles when deployed in this domain, partly due to their training on discipline-agnostic data. Accordingly, there is a recent uptick of derivative ME-specific LLMs being reported. As the research community shifts towards these new LLM-centric solutions for ME-related problems, the shift compels a deeper look at the diffusion of LLMs in this emerging landscape. Consequently, this review consolidates the diversity of ME-tailored LLMs use cases and identifies the supportive technical stacks associated with these implementations. Broadly, the review demonstrates how various categories of LLMs are re-shaping concrete aspects of engineering design, manufacturing and applied mechanics. At a more specific level, it uncovered emerging LLMs’ role in boosting the intelligence of digital twins, enriching bidirectional communication within the human-cyber-physical infrastructure, advancing the development of intelligent process planning in manufacturing and facilitating inverse mechanics. It further spotlights the coupling of LLMs with other generative models for promoting efficient computer-aided conceptual design, prototyping, knowledge discovery and creativity. Finally, it revealed training modalities/infrastructures necessary for developing ME-specific language models, discussed LLMs' features that are incongruent with typical engineering workflows, and concluded with prescriptive approaches to mitigate impediments to the progressive adoption of LLMs as part of advanced intelligent solutions.}
}
@article{RADU2023100008,
title = {Charting opportunities and guidelines for augmented reality in makerspaces through prototyping and co-design research},
journal = {Computers & Education: X Reality},
volume = {2},
pages = {100008},
year = {2023},
issn = {2949-6780},
doi = {https://doi.org/10.1016/j.cexr.2023.100008},
url = {https://www.sciencedirect.com/science/article/pii/S2949678023000028},
author = {Iulian Radu and Josia Yuan and Xiaomeng Huang and Bertrand Schneider},
keywords = {Augmented reality, Makerspaces, Co-design, STEM, Classroom integration},
abstract = {Makerspace environments are becoming popular project-based learning spaces where students interact with physical objects and peer collaboration, while developing 21st century skills and engaging with science, technology, engineering, and math (STEM) topics. At the same time, augmented reality (AR) technology, which combines physical objects with digital visualizations, is becoming increasingly applicable for makerspace activities and has potential to address challenges for student learning in makerspaces. However, there is a lack of understanding of how to use and integrate AR in real makerspace environments. In this research we use a co-design methodology to address the following questions: (1) How can AR be useful for education in makerspaces? (2) How are students impacted by the process of co-designing AR technology? and (3) What are practical considerations for integrating AR in makerspaces? We engaged in a co-design process in a semester-long makerspace course attended by 18 students in a graduate school of education. Through this process, we generated six prototypes with seven student co-designers, exploring AR use in design, fabrication, programming, electronics, and training. We also identified areas where AR technology can benefit makerspaces, such as teaching STEM skills, facilitating construction activities, enhancing contextualization of learning, and debugging. We observed that students participating in co-design demonstrated improved understanding of technology design, enthusiasm for engaging with makerspaces and AR, and increased critical thinking about AR technology. These results suggest considerations and guidelines for integrating AR technology into makerspace environments.}
}
@article{2024i,
title = {Table of Contents},
journal = {The Journal of Pharmacology and Experimental Therapeutics},
volume = {389},
pages = {i-xxvi},
year = {2024},
issn = {0022-3565},
doi = {https://doi.org/10.1016/S0022-3565(24)17260-6},
url = {https://www.sciencedirect.com/science/article/pii/S0022356524172606}
}
@article{WU2026100817,
title = {New paradigm of distributed artificial intelligence for LLM implementation and its key technologies},
journal = {Computer Science Review},
volume = {59},
pages = {100817},
year = {2026},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2025.100817},
url = {https://www.sciencedirect.com/science/article/pii/S1574013725000930},
author = {Yijin Wu and Zirun Li and Bingrui Guo and Shanshan He and Bijing Liu and Xiaojie Liu and Shan He and Donghui Guo},
keywords = {Distributed artificial intelligence, Cloud computing, Caching, Load-balancing, Reasoning, LLM},
abstract = {With the Internet’s development and information technology advancement, current network applications and services, such as e-commerce, industrial automation, and vehicular automation, have experienced substantial expansion. Foundation models, represented by large language models (LLMs), have emerged in response to growing demands. Their broad range of applications has brought significant advancements to various industries. While such developments have improved people’s economic lives and social activities, the challenges posed by the rapid growth of data volume and network traffic cannot be overlooked. Intelligent systems aimed at enhancing knowledge computation and learning capabilities are gradually gaining attention. Nevertheless, efficient and flexible intelligent systems are still in their early stages, leaving ample space for further optimization. This study provides an overview of Distributed Artificial Intelligence (DAI) with its related paradigm, briefly introduces the evolution of LLMs, and proposes a novel optimization framework named PCD Tri-Tuning for DAI workflows: leveraging caching-related technologies to enhance perceptual capabilities, adopting load-balancing techniques for computational optimization, and developing reasoning methodologies and cooperation techniques to improve decision-making. Subsequently, the study examines the pivotal role of the proposed optimization framework in practical domains such as e-commerce, smart manufacturing, and vehicular automation while also discussing the challenges and outlining strategies for further development.}
}
@article{HAGL2024101000,
title = {Change management interventions: Taking stock and moving forward},
journal = {Human Resource Management Review},
volume = {34},
number = {1},
pages = {101000},
year = {2024},
issn = {1053-4822},
doi = {https://doi.org/10.1016/j.hrmr.2023.101000},
url = {https://www.sciencedirect.com/science/article/pii/S1053482223000530},
author = {Christina Hagl and Rouven Kanitz and Katerina Gonzalez and Martin Hoegl},
keywords = {Change management interventions, Change implementation, Change support, Change leadership, Change management},
abstract = {Change management interventions (CMIs) are intentional activities that managers employ to facilitate planned organizational change by influencing employee receptivity to and adoption of that change. CMIs have been unclearly conceptualized and empirical insights on CMIs have become disjointed across research communities, limiting our understanding of the nature and effects CMIs can have. To address this shortcoming, we integrate and build on existing frameworks to provide an overview of the empirically studied CMI types, their mechanisms, and their outcomes. From our review of 119 empirical studies, we find that there are six overarching CMI types (and 14 sub-types): (1) communication (informing, framing, dialogic), (2) support (training, coaching, organizational change support), (3) involvement (consulting, co-creating, co-deciding), (4) reinforcement (rewards and goal-setting), (5) social influence (role modeling and peer exchange), and (6) coercion. Furthermore, based on our results, we encourage researchers to continue to strengthen an intervention-focused and context-sensitive approach to organizational change in the following underexplored areas: conceptualizing and measuring CMIs, bundles and interactions of CMIs, boundary conditions of CMIs, unintended consequences of CMIs, and the use of digital technology to enhance CMIs.}
}
@article{SOME2024103486,
title = {Heterogeneity in the competition-cost of equity relation},
journal = {International Review of Economics & Finance},
volume = {95},
pages = {103486},
year = {2024},
issn = {1059-0560},
doi = {https://doi.org/10.1016/j.iref.2024.103486},
url = {https://www.sciencedirect.com/science/article/pii/S1059056024004787},
author = {Hyacinthe Y. Somé and Pascale Valéry},
keywords = {Competition, Agency costs, Implied cost of equity, Firm size, Free cash flow},
abstract = {We explore the effect of firm heterogeneity on the implied cost of equity. To this end, we exploit two opposing effects of competition on the cost of equity: its negative effect on a firm's exposure to systematic risk and its positive effect on a firm's agency costs when it acts as a managerial disciplining device. Using a U.S. sample comprising 4764 firms from 1986 to 2017, we find that, on average, competition reduces equity costs by diminishing managerial expropriation, but increases these costs for small and distressed firms as they are more exposed to systematic risk. This agency costs link holds in developed countries only.}
}
@article{RAZAQUE2025100789,
title = {A comprehensive review of cybersecurity vulnerabilities, threats, and solutions for the Internet of Things at the network-cum-application layer},
journal = {Computer Science Review},
volume = {58},
pages = {100789},
year = {2025},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2025.100789},
url = {https://www.sciencedirect.com/science/article/pii/S1574013725000656},
author = {Abdul Razaque and Salim Hariri and Abrar M. Alajlan and Joon Yoo},
keywords = {Internet of Things, Cyber security, Real-time, Controlling system, Data mining, Scientific decision-making system, Vulnerabilities, Security attacks},
abstract = {The proliferation of smart homes, smart logistics, and other technologies has expedited the expansion of Internet-of-Things (IoT) devices. This expansion has heightened the complexity of associated security challenges. Despite extensive research on IoT security, several studies fail to provide a comprehensive examination of both the network and application layers. This is particularly applicable to real-time and mission-critical settings. This review addresses that deficiency by offering a systematic review of IoT across five tiers. It concentrates on the application layer, categorizing it into three domains: real-time control systems, scientific decision-making systems, and query/scan search systems. The study examines vulnerabilities, attack vectors, and security measures in real-time control and query/scan systems. It examines how emerging technologies such as artificial intelligence (AI), Software Defined Networking (SDN), and fog/edge computing can enhance security via improved context awareness and access management. The study ultimately presents recommendations and suggests enhancements to foster trust, scalability, and enhanced security in contemporary IoT systems.}
}
@article{CASHEEKAR2024100632,
title = {A contemporary review on chatbots, AI-powered virtual conversational agents, ChatGPT: Applications, open challenges and future research directions},
journal = {Computer Science Review},
volume = {52},
pages = {100632},
year = {2024},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2024.100632},
url = {https://www.sciencedirect.com/science/article/pii/S1574013724000169},
author = {Avyay Casheekar and Archit Lahiri and Kanishk Rath and Kaushik Sanjay Prabhakar and Kathiravan Srinivasan},
keywords = {Computational intelligence, Artificial intelligence, Chatbots, Conversational agents, ChatGPT},
abstract = {This review paper offers an in-depth analysis of AI-powered virtual conversational agents, specifically focusing on OpenAI’s ChatGPT. The main contributions of this paper are threefold: (i) an exhaustive review of prior literature on chatbots, (ii) a background of chatbots including existing chatbots/conversational agents like ChatGPT, and (iii) a UI/UX design analysis of prominent chatbots. Another contribution of this review is the comprehensive exploration of ChatGPT’s applications across a multitude of sectors, including education, business, public health, and more. This review highlights the transformative potential of ChatGPT, despite the challenges it faces such as hallucination, biases in training data, jailbreaks, and anonymous data collection. The review paper then presents a comprehensive survey of prior literature reviews on chatbots, identifying gaps in the prior work and highlighting the need for further research in areas such as chatbot evaluation, user experience, and ethical considerations. The paper also provides a detailed analysis of the UI/UX design of prominent chatbots, including their conversational flow, visual design, and user engagement. The paper also identifies key future research directions, including mitigating language bias, enhancing ethical decision-making capabilities, improving user interaction and personalization, and developing robust governance frameworks. By solving these issues, we can ensure that AI chatbots like ChatGPT are used responsibly and effectively across a broad variety of applications. This review will be a valuable resource for researchers and practitioners in understanding the current state and future potential of AI chatbots like ChatGPT.}
}
@article{RAZA2025108144,
title = {A comprehensive survey of Network Digital Twin architecture, capabilities, challenges, and requirements for Edge–Cloud Continuum},
journal = {Computer Communications},
volume = {236},
pages = {108144},
year = {2025},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2025.108144},
url = {https://www.sciencedirect.com/science/article/pii/S014036642500101X},
author = {Syed Mohsan Raza and Roberto Minerva and Noel Crespi and Maira Alvi and Manoj Herath and Hrishikesh Dutta},
keywords = {Edge-Cloud Continuum, Network Digital Twin, Componentization, Containerization, Segment, Microservices, Data modeling, Learning models, Simulation},
abstract = {Network Digital Twin (NDT) collects data from physical, virtual, and software components and supports real-time network performance analysis, emulation, and intelligent physical network control. This paper surveys the current state of NDT specifications and explores NDT benefits for Network Operators (NOs) and its possible roles in future network management. It discusses the NDT key components, architecture, and integration of Machine Learning and Artificial Intelligence models in the NDT. Further, it covers virtualization technology management, suitability of Software-Defined Networking capabilities, and simulation tools to empower NDT. Two perspectives make the position of this survey different from existing studies; first, it highlights NDT limitations regarding Edge–Cloud Continuum (ECC) contextualization. ECC is a purposeful trending integration of Edge and Cloud Computing, involving multiple stakeholders like Service Providers, Customers, and Platform or Infrastructure Providers. However, current NDT specifications have not mentioned the ways to benefit stakeholders other than NOs. We also discuss notable computing and communication technologies transformations necessary to consider during NDT modeling, the existing data models, and reusable vocabularies that can be extended to achieve a detailed ECC representation for all stakeholders, essentially for Service Providers and Customers. Secondly, a data model is proposed that covers descriptive and prescriptive features and aims to provide a granular representation of ECC components to meet stakeholders’ requirements and render particular user information views. Different explored NDT perspectives, and proposed data model reduces the impact of existing NDT limitations in ECC representation.}
}
@article{IGNACZ2025123256,
title = {Machine learning for the advancement of membrane science and technology: A critical review},
journal = {Journal of Membrane Science},
volume = {713},
pages = {123256},
year = {2025},
issn = {0376-7388},
doi = {https://doi.org/10.1016/j.memsci.2024.123256},
url = {https://www.sciencedirect.com/science/article/pii/S0376738824008500},
author = {Gergo Ignacz and Lana Bader and Aron K. Beke and Yasir Ghunaim and Tejus Shastry and Hakkim Vovusha and Matthew R. Carbone and Bernard Ghanem and Gyorgy Szekely},
keywords = {Deep learning, Predictive models, Generative models, Molecular modeling, Cheminformatics},
abstract = {Machine learning (ML) has been rapidly transforming the landscape of natural sciences and has the potential to revolutionize the process of data analysis and hypothesis formulation as well as expand scientific knowledge. ML has been particularly instrumental in the advancement of cheminformatics and materials science, including membrane technology. In this review, we analyze the current state-of-the-art membrane-related ML applications from ML and membrane perspectives. We first discuss the ML foundations of different algorithms and design choices. Then, traditional and deep learning methods, including application examples from the membrane literature, are reported. We also discuss the importance of learning data and both molecular and membrane-system featurization. Moreover, we follow up on the discussion with examples of ML applications in membrane science and technology. We detail the literature using data-driven methods from property prediction to membrane fabrication. Various fields are also discussed, such as reverse osmosis, gas separation, and nanofiltration. We also differentiate between downstream predictive tasks and generative membrane design. Additionally, we formulate best practices and the minimum requirements for reporting reproducible ML studies in the field of membranes. This is the first systematic and comprehensive review of ML in membrane science.}
}
@article{JEGATHEESAN2024137648,
title = {Modeling the properties of terminal blend crumb rubber modified bitumen with crosslinking additives},
journal = {Construction and Building Materials},
volume = {444},
pages = {137648},
year = {2024},
issn = {0950-0618},
doi = {https://doi.org/10.1016/j.conbuildmat.2024.137648},
url = {https://www.sciencedirect.com/science/article/pii/S0950061824027909},
author = {N. Jegatheesan and Mohd Rasdan Ibrahim and Ali Najah Ahmed and Suhana Koting and Ahmed El-Shafie and Herda Yati Binti Katman},
keywords = {Machine-learning algorithms, Prediction models, Terminal blend-crumb rubber modified bitumen, Crosslinking additive, Composite modification, High interaction parameters},
abstract = {This study aimed to develop models assessing 26 machine-learning algorithms in regression analysis to predict the properties of terminal blend crumb rubber-modified bitumen (TB-CRMB) made with crosslinking additives. During the data collection, the properties of the modified binders prepared at 6, 10 and 14% of crumb rubber (CR), considering three types of modifications and eighteen blending scenarios with different interaction factors, were assessed in terms of penetration, softening point, rotational viscosity, storage stability, rheological parameters, and rutting and fatigue factors. Results showed that the Matern 5/2 Gaussian Process Regression (GPR) model demonstrated efficient performance in predicting physical, viscoelastic, rutting, and fatigue properties whereas wide artificial neural networks exhibited enhanced accuracy in predicting storage stability and rotational viscosity. The results also suggest that it is feasible to implement a single type of model developed using the Matern 5/2 GPR algorithm for accurately predicting all the TB-CRMB properties considered. The best models demonstrated that crosslinking additives significantly influenced TB-CRMB production and performance. In TB-CRMB production, sulfur as a crosslinking additive showed better compatibility than trans-polyoctenamer-rubber and significantly reduced interaction temperatures at lower CR content, leading to energy savings compared to the traditional TB production.}
}
@article{BAASSIRI2025125204,
title = {CFD modelling and simulations of atomization-based processes for production of drug particles: A review},
journal = {International Journal of Pharmaceutics},
volume = {670},
pages = {125204},
year = {2025},
issn = {0378-5173},
doi = {https://doi.org/10.1016/j.ijpharm.2025.125204},
url = {https://www.sciencedirect.com/science/article/pii/S0378517325000407},
author = {Mohamad Baassiri and Vivek Ranade and Luis Padrela},
keywords = {CFD, Drug nanoparticles, Atomization, Computational modelling},
abstract = {Atomization-based techniques are widely used in pharmaceutical industry for production of fine drug particles due to their versatility and adaptability. Key performance measure of such techniques is their ability to provide control over critical quality attributes (CQAs) of produced drug particles. CQAs of drug particles produced via atomization critically depend on fluid dynamics of sprays; resulting mixing, heat and mass transfer; distribution of supersaturation and subsequent nucleation and growth of particles. It is essential to develop and use computational fluid dynamics (CFD) models for adequate understanding of multi-scale transport processes ranging from molecular scale mixing and particle scale processes, and from atomizer nozzle to overall spray chamber scale establishing relationships between CQAs and design and operating parameters of spray nozzle and chamber. In this work, we critically review past and current research efforts on CFD modelling of pharmaceutical atomization-based processes with an objective to provide clear assessment of the state of the art and to provide recommendations. An overview of the key atomization-based methods for producing drug particles with desired CQAs is presented. Key underlying physical processes and relevant concepts are then outlined. This discussion is related to the demands on CFD models; and state of the art is then discussed with respect to the process needs. Recommendations are provided towards higher fidelity and more efficient models of atomized multiphase flow dynamics and turbulence, drying modelling for the produced particles, and validation approaches. We conclude by highlighting a perceived need for numerical atomization studies with a pharmaceutical context; then, we deliver an outlook on current promising active control and machine learning strategies to augment the shift towards quality-by-design approaches in pharmaceutical manufacturing.}
}
@article{LEE2024100649,
title = {Data literacy of principals in K–12 school contexts: A systematic review},
journal = {Educational Research Review},
volume = {45},
pages = {100649},
year = {2024},
issn = {1747-938X},
doi = {https://doi.org/10.1016/j.edurev.2024.100649},
url = {https://www.sciencedirect.com/science/article/pii/S1747938X24000587},
author = {Jihyun Lee and Dennis Alonzo and Kim Beswick and Cherry Zin Oo and Daniel W.J. Anson and Jan Michael Vincent Abril},
keywords = {Principals, Teachers, Data literacy, Data use, Indicators, Dimensions, Decision-making},
abstract = {This systematic review aims to clarify the concept of principals' data literacy and its various components. After examining 56 empirical studies, we have defined principals' data literacy and identified 63 specific indicators, organized into seven dimensions. Our findings highlight the complex tasks and responsibilities principals undertake to effectively lead with data. Although some data-related activities overlap between principals and teachers, the nature, scope, and purposes of data use differ between these roles. While teachers’ data literacy focuses on hands-on data creation, collection, and analysis, principals' data literacy revolves around leading their school communities and beyond. We argue that three of the seven dimensions—Dimension 3 (“Data use for fostering a data-driven culture”), Dimension 4 (“Data use for school improvement”), and Dimension 5 (“Data use for informing own practices”)—are particularly relevant to school leaders, thereby distinguishing principals' data literacy from that of teachers. We conclude by suggesting several practical implications based on our review, which could benefit the professional development of both school leaders and teachers at various career stages.}
}
@article{MERVIN2021474,
title = {Uncertainty quantification in drug design},
journal = {Drug Discovery Today},
volume = {26},
number = {2},
pages = {474-489},
year = {2021},
issn = {1359-6446},
doi = {https://doi.org/10.1016/j.drudis.2020.11.027},
url = {https://www.sciencedirect.com/science/article/pii/S1359644620305110},
author = {Lewis H. Mervin and Simon Johansson and Elizaveta Semenova and Kathryn A. Giblin and Ola Engkvist},
abstract = {Machine learning and artificial intelligence are increasingly being applied to the drug-design process as a result of the development of novel algorithms, growing access, the falling cost of computation and the development of novel technologies for generating chemically and biologically relevant data. There has been recent progress in fields such as molecular de novo generation, synthetic route prediction and, to some extent, property predictions. Despite this, most research in these fields has focused on improving the accuracy of the technologies, rather than on quantifying the uncertainty in the predictions. Uncertainty quantification will become a key component in autonomous decision making and will be crucial for integrating machine learning and chemistry automation to create an autonomous design–make–test–analyse cycle. This review covers the empirical, frequentist and Bayesian approaches to uncertainty quantification, and outlines how they can be used for drug design. We also outline the impact of uncertainty quantification on decision making.}
}
@article{VANDENBROEK2024102411,
title = {When strategy is a dirty word: The role of visuals in sensegiving strategy to a skeptical audience},
journal = {Long Range Planning},
volume = {57},
number = {1},
pages = {102411},
year = {2024},
issn = {0024-6301},
doi = {https://doi.org/10.1016/j.lrp.2023.102411},
url = {https://www.sciencedirect.com/science/article/pii/S0024630123001188},
author = {Antonius {van den Broek} and Jonathan Gander},
keywords = {Sensegiving, Strategy presentations, Skepticism, Visual communication},
abstract = {When setting a new strategy for their firm, managers engage in a range of sensegiving activities designed to introduce the new direction and explain the reasons for the change. These communication events commonly involve the use of strategic management terms and concepts to explain and justify the prescribed strategy. Literature thus far assumes that audiences understand and agree that these terms and underlying concepts are appropriate and relevant. Yet such views fail to explain strategy sensegiving in contexts where audiences of strategy presentations are ignorant or skeptical towards strategy concepts and ideas. We examine sensegiving under such conditions by analyzing a manager introducing a new strategy in a creative agency which expressed skepticism towards the concepts and practice of strategizing. Using data from video recordings of a sequence of internal strategy presentations, we identify three strategies designed to overcome prejudice towards strategic thinking while at the same time encouraging its use: winning the right to lead, finding resonance, and enrolling the audience into the strategy. We further find how these three sensegiving strategies are supported by carefully crafted visuals to either emphasize or de-emphasize aspects of the strategy and its supporting rationale. Our findings extend the literature on the practice of strategy by illustrating how the visual supports sensegiving efforts to guide a firm's interpretation of a proposed new strategic direction.}
}
@article{TAHERKHANI2023848,
title = {On the application of in-situ monitoring systems and machine learning algorithms for developing quality assurance platforms in laser powder bed fusion: A review},
journal = {Journal of Manufacturing Processes},
volume = {99},
pages = {848-897},
year = {2023},
issn = {1526-6125},
doi = {https://doi.org/10.1016/j.jmapro.2023.05.048},
url = {https://www.sciencedirect.com/science/article/pii/S1526612523005212},
author = {Katayoon Taherkhani and Osazee Ero and Farima Liravi and Sahar Toorandaz and Ehsan Toyserkani},
keywords = {Additive manufacturing, Laser powder bed fusion, In-situ sensors, In-situ monitoring, Machine learning},
abstract = {Laser powder bed fusion (LPBF) is one class of metal additive manufacturing (AM) used to fabricate high-quality complex-shape components. This technology has significantly progressed over the last several years allowing the fabrication of high-value components for a broad range of applications, normally unmatched by other metal AM processes. However, the full adoption of LPBF to serial production is still challenging due to several barriers such as repeatability and reliability of final product quality. The main obstacle could be the high sensitivity of LPBF to environmental and process disturbances. Additionally, LPBF is governed by many process parameters. These factors profoundly affect the process, causing defects formation. To achieve high quality parts, trial and errors are conventionally carried out to obtain optimum parameters that result in good quality for a specific application. However, in recent years attention to the development of quality assurance platforms in LPBF has been the cornerstone of research and development. To this end, researchers have proceeded with three steps: 1) Gaining knowledge from the process by installing in-situ sensing equipment and collecting information from the process. 2) Understanding how the print parameters affect the process, analyzing in-situ datasets and developing defect detection algorithms, and 3) Developing real-time closed-loop control systems using the detection algorithms of Step 2 to automatically adjust the undesired phenomena in the process by changing the print parameters. Although valuable studies were published for the two first steps, the development of real-time controllers has remained challenging. Thus, this study aims to critically review the two first steps to provide insights for researchers into moving toward the development of the control system. In this study, in-situ sensing devices implemented in LPBF are categorized, explained in detail, and mapped to the literature. Then, a comprehensive review is conducted on the latest machine learning (ML) algorithms applied to the in-situ data of LPBF, such as supervised learning, unsupervised learning, and reinforcement learning. Additionally, a comprehensive discussion is provided on in-situ sensors and ML methods applied to LPBF. Lastly, this article specifies trends and future research outlook on this topic.}
}
@article{LIU2024100985,
title = {Thin-walled deployable composite structures: A review},
journal = {Progress in Aerospace Sciences},
volume = {146},
pages = {100985},
year = {2024},
issn = {0376-0421},
doi = {https://doi.org/10.1016/j.paerosci.2024.100985},
url = {https://www.sciencedirect.com/science/article/pii/S0376042124000113},
author = {Tian-Wei Liu and Jiang-Bo Bai and Nicholas Fantuzzi and Xiang Zhang},
keywords = {Deployable composite structure, Thin-walled, Boom, Tape-spring, Hinge, Reflector, Optimization},
abstract = {The elastic strain energy-driven thin-walled deployable composite structures, characterized by their integration of structure and functionality, have attracted considerable attention in the field of space applications. These structures utilize the stored strain energy accumulated during the folding process to achieve elastic deployment. Significant progress has been made in the understanding of deformation mechanisms, modeling, design, optimization, and applications of such structures based on existing research. This review critically discusses over 300 papers from the past few decades, providing a comprehensive exploration of the development of three representative types of deployable composite structures: deployable composite hinges, booms, and reflectors. Specifically, it starts by reviewing the structural design, functional mechanisms, theories, finite element modeling methods and experimental investigations for these three types of structures. It then introduces optimization design methods and their applications in deployable composite structures. Additionally, specific practical application cases of deployable composite structures are discussed. Finally, future challenges and prospects for deployable composite structures are outlined. This paper serves as a valuable reference and inspiration for the design and application of deployable composite structures. It is expected to promote further advancements in this field.}
}
@article{GOLDSCHMIDT2025104510,
title = {Network intrusion datasets: A survey, limitations, and recommendations},
journal = {Computers & Security},
volume = {156},
pages = {104510},
year = {2025},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2025.104510},
url = {https://www.sciencedirect.com/science/article/pii/S0167404825001993},
author = {Patrik Goldschmidt and Daniela Chudá},
keywords = {Network intrusion detection, NIDS, Machine learning for intrusion detection, Cybersecurity datasets, NIDS best practices, Comparative dataset analysis},
abstract = {Data-driven cyberthreat detection has become a crucial defense technique in modern cybersecurity. Network defense, supported by Network Intrusion Detection Systems (NIDSs), has also increasingly adopted data-driven approaches, leading to greater reliance on data. Despite the importance of data, its scarcity has long been recognized as a major obstacle in NIDS research. In response, the community has published many new datasets recently. However, many of them remain largely unknown and unanalyzed, leaving researchers uncertain about their suitability for specific use cases. In this paper, we aim to address this knowledge gap by performing a systematic literature review (SLR) of 89 public datasets for NIDS research. Each dataset is comparatively analyzed across 13 key properties, and its potential applications are outlined. Beyond the review, we also discuss domain-specific challenges and common data limitations to facilitate a critical view on data quality. To aid in data selection, we conduct a dataset popularity analysis in contemporary state-of-the-art NIDS research. Furthermore, the paper presents best practices for dataset selection, generation, and usage. By providing a comprehensive overview of the domain and its data, this work aims to guide future research toward improving data quality and the robustness of NIDS solutions.}
}
@article{2025S729,
title = {Subject Index},
journal = {Gastrointestinal Endoscopy},
volume = {101},
number = {5, Supplement },
pages = {S729-S756},
year = {2025},
note = {ASGE Abstracts - DDW 2025},
issn = {0016-5107},
doi = {https://doi.org/10.1016/j.gie.2025.03.150},
url = {https://www.sciencedirect.com/science/article/pii/S0016510725003190}
}
@article{DOCANTO2024127320,
title = {Analyzing and predicting the response of the signal grass seed crop to plant nitrogen status},
journal = {European Journal of Agronomy},
volume = {160},
pages = {127320},
year = {2024},
issn = {1161-0301},
doi = {https://doi.org/10.1016/j.eja.2024.127320},
url = {https://www.sciencedirect.com/science/article/pii/S1161030124002417},
author = {Marcos Weber {do Canto} and Taise Robinson Kunrath and Antonio Carlos {Saraiva da Costa} and Marco {dos Santos Martinez} and Gleice Menezes {de Almeida} and Hugo Zeni Neto and João Luiz Pratt Daniel},
keywords = {Nitrogen status indicators, Nitrogen, Nitrogen remobilization, Seed growth},
abstract = {Nitrogen (N) deficiency has detrimental effects on productivity and the profit of producers in areas where signal grass [Urochloa decumbens (Stapf) R.D. Webster (syn. Brachiaria decumbens Stapf.)] cv. Basilisk is grown for seed production. The objective of this paper was to clarify the effects of indicators of signal grass plant N status on seed yield (SY), SY components, yield formation, seed quality, panicle growth parameters, and remobilization of vegetative N on seed growth. Germinable pure SY, harvest index (HI), and N harvest index (NHI) were also measured. Different rates of N fertilizer application (0, 50, 100, and 150 kg ha−1) were applied after the cleaning cut to both the first crop (October - January) and the second crop (February - May) in 2010–2011 and 2011–2012, on a sandy loam soil representative of soils used for seed production in Brazil. Although the N nutrition index (NNI) increased at key developmental stages, the highest values were near to 0.85. This suggests that all crops were maintained under N-limiting conditions. In N-limited crops, a strong relationship was detected between NNI and accumulated N deficit throughout the study period with relative SY. A low NNI after the cleaning cut was found to restrict fertile tiller number (FTN), spikelets per panicle, and spikelet density m−2 measured at anthesis. In all crops, at harvest, NNI at anthesis increased germinable pure SY, FTN, number of seeds per panicle, HI, NHI, and amount of remobilized N to seeds, but not thousand seed weight (TSW), seed germination, panicle dry matter (DM) accumulation rate, and individual seed growth rate. Regression analyses suggested that the NNI, accumulated N deficit, aboveground plant biomass (AGPB), and N content were better associated with relative SY than with plant N concentration (PNC). The study shows that the NNI quantifies the intensity and duration of N deficiency in signal grass and should be considered in research studies and for application in seed production fields to improve N fertilization recommendations.}
}
@article{GIORDANO2025104186,
title = {Decomposing maintenance actions into sub-tasks using natural language processing: A case study in an Italian automotive company},
journal = {Computers in Industry},
volume = {164},
pages = {104186},
year = {2025},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2024.104186},
url = {https://www.sciencedirect.com/science/article/pii/S0166361524001143},
author = {Vito Giordano and Gualtiero Fantoni},
keywords = {Natural language processing, Text mining, Maintenance work order, Industrial applications, Association rule mining, Large language model},
abstract = {Industry 4.0 has led to a huge increase in data coming from machine maintenance. At the same time, advances in Natural Language Processing (NLP) and Large Language Models provide new ways to analyse this data. In our research, we use NLP to analyse maintenance work orders, and specifically the descriptions of failures and the corresponding repair actions. Many NLP studies have focused on failure descriptions for categorising them, extracting specific information about failure, or supporting failure analysis methodologies (such as FMEA). Whereas, the analysis of repair actions and its relationship with failure remains underexplored. Addressing this gap, our study makes three significant contributions. Firstly, we focused on the Italian language, which presents additional challenges due to the dominance of NLP systems that are mainly designed for English. Secondly, it proposes a method for automatically subdividing a repair action into a set of sub-tasks. Lastly, it introduces an approach that employs association rule mining to recommend sub-tasks to maintainers when addressing failures. We tested our approach with a case study from an automotive company in Italy. The case study provides insights into the current barriers faced by NLP applications in maintenance, offering a glimpse into the future opportunities for smart maintenance systems.}
}
@article{TREANTA2024100288,
title = {Efficiency criteria and dual techniques for some nonconvex multiple cost minimization models},
journal = {IFAC Journal of Systems and Control},
volume = {30},
pages = {100288},
year = {2024},
issn = {2468-6018},
doi = {https://doi.org/10.1016/j.ifacsc.2024.100288},
url = {https://www.sciencedirect.com/science/article/pii/S246860182400049X},
author = {Savin Treanţă and Ramona-Manuela Calianu},
keywords = {Efficiency criteria, Dual techniques, Nonconvex multiple cost minimization models},
abstract = {In this study, we investigate a class of multi-objective variational control problems governed by nonconvex simple integral functionals. Concretely, we establish and prove (necessary and sufficient) efficiency criteria and dual techniques for some nonconvex multiple cost minimization models. To this aim, we extend and use the concept of (Ψ,ω)-invexity to the case of multi-objective variational control problems. Thereafter, by assuming (Ψ,ω)-invexity, (strictly) (Ψ,ω)-pseudoinvexity and/or (Ψ,ω)-quasiinvexity of the involved functionals, we state the sufficient efficiency criteria and associate a dual problem for the considered model.}
}
@article{JAIPRAKASH2025110194,
title = {Exploring text-to-image generation models: Applications and cloud resource utilization},
journal = {Computers and Electrical Engineering},
volume = {123},
pages = {110194},
year = {2025},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2025.110194},
url = {https://www.sciencedirect.com/science/article/pii/S0045790625001375},
author = {Sahani Pooja Jaiprakash and Choudhary Shyam Prakash},
keywords = {Text-to-image generation, VAE, GAN, StackGAN, Diffusion model, Cloud resouce utilization, Edge-cloud computing},
abstract = {Generating images from text presents a significant challenge in computer vision. Moreover, manually acquiring images from multiple perspectives for object or product generation is a resource-intensive and expensive endeavor. However, recent breakthroughs in deep learning and artificial intelligence have opened doors to creating new images from diverse data sources, and cloud resources play a pivotal role in alleviating the resource-intensive nature of this endeavor. As a result, substantial research efforts have been directed toward advancing image generation techniques, yielding impressive results. This paper aims to provide a comprehensive overview of existing image generation methods, offering insights into this evolving field of text-to-image generation. It traces the historical development of this technology. It examines the key models that have shaped its evolution, including Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), Conditional GANs (CGANs), StackGAN, Transformers, and diffusion models. The paper offers insights into the functioning of text-to-image generation within the GAN architecture, elucidating the mechanisms behind transforming textual descriptions into visual content. Additionally, the integration of text-to-image generation with cloud and edge-cloud computing highlights the synergistic potential of these technologies while addressing the challenges and considerations associated with cloud infrastructure. The paper concludes by surveying the diverse applications of text-to-image generation across various domains, such as art, e-commerce, entertainment, and education. It also discusses the evaluation metrics commonly used in assessing the quality of generated images and the challenges that exist both within the methods and in their application across different domains. This review offers a comprehensive overview of the capabilities and limitations of text-to-image generation. Also, we have introduced a new HiResGAN model using a single generator/discriminator pair of networks to produce high-resolution 256 × 256 images from textual descriptions. We illustrate the efficacy of our model in producing high-resolution images based on contextually rich text descriptions that are visually plausible and semantically consistent through a series of experiments and analyses.}
}
@incollection{GALITSKY2025283,
title = {Chapter 8 - Identifying large language model hallucinations in health communication},
editor = {Boris Galitsky},
booktitle = {Healthcare Applications of Neuro-Symbolic Artificial Intelligence},
publisher = {Academic Press},
pages = {283-329},
year = {2025},
isbn = {978-0-443-30046-2},
doi = {https://doi.org/10.1016/B978-0-443-30046-2.00012-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780443300462000120},
author = {Boris Galitsky},
keywords = {Large language model hallucination, fact-checking, handling inconsistent verification sources, collaborative iterative mode, syntax-semantic alignment},
abstract = {Large language models (LLMs) sometimes generate texts plagued with inaccuracies and fabricated information. We present a fact-checking system known as “Truth-O-Meter” which detects erroneous facts by cross-referencing generated content with information from the web and reputable sources and then offers appropriate corrections. We employ text mining and web mining techniques to pinpoint accurate corresponding sentences and to employ a syntactic and semantic generalization process to enhance content quality. To effectively handle the challenges posed by inconsistent information sources during fact-checking, we employ an argumentation-analysis framework based on defeasible logic programming. In a comparative evaluation with competitive approaches that rely on reinforcement learning integrated with LLM or token-based hallucination detection, our fact-checking engine demonstrates significant enhancements in the factual accuracy and meaningfulness of LLM-generated content.11https://github.com/bgalitsky/Truth-O-Meter-Making-ChatGPT-Truthful.}
}
@incollection{GREEN20241,
title = {Chapter One - Narrative transportation: How stories shape how we see ourselves and the world},
editor = {Bertram Gawronski},
series = {Advances in Experimental Social Psychology},
publisher = {Academic Press},
volume = {70},
pages = {1-82},
year = {2024},
issn = {0065-2601},
doi = {https://doi.org/10.1016/bs.aesp.2024.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S0065260124000145},
author = {Melanie C. Green and Markus Appel},
keywords = {Narrative, Story, Transportation, Character identification, Attitudes, Persuasion, Emotion, Belongingness, Immersion, Theory of mind},
abstract = {Scientific interest in the processing and effects of narrative information has substantially increased in recent years. The focus of this chapter is on narrative transportation, an experiential state of immersion in which all mental processes are concentrated on the events occurring in the narrative. We describe and integrate interdisciplinary advances in the study of narrative transportation. After an introduction of the concept and related approaches, we outline antecedents in terms of story factors, individual differences, situational variables, and related interactions. In the following sections, we introduce processes and effects that are facilitated by stories and narrative transportation. This includes research on persuasion, misinformation and its correction, self and identity, social cognitive skills, and the fulfillment of belongingness needs. We close with an outlook on the role of technology and artificial intelligence, meaning making, and climate change communication as emerging and future directions.}
}
@incollection{2024552,
editor = {David Baker and Lucy Ellis},
booktitle = {Encyclopedia of Libraries, Librarianship, and Information Science (First Edition)},
publisher = {Academic Press},
edition = {First Edition},
address = {Oxford},
pages = {552-598},
year = {2024},
isbn = {978-0-323-95690-1},
doi = {https://doi.org/10.1016/B978-0-323-95689-5.09001-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780323956895090015}
}
@article{MORCIANO2025100862,
title = {Trending applications of Phase Change Materials in sustainable thermal engineering: An up-to-date review},
journal = {Energy Conversion and Management: X},
volume = {25},
pages = {100862},
year = {2025},
issn = {2590-1745},
doi = {https://doi.org/10.1016/j.ecmx.2024.100862},
url = {https://www.sciencedirect.com/science/article/pii/S2590174524003404},
author = {Matteo Morciano and Matteo Fasano and Eliodoro Chiavazzo and Luigi Mongibello},
keywords = {Phase change materials, Energy storage, Sustainable thermal management, Solar thermal energy, Smart textiles},
abstract = {The on-going search for increasingly sustainable and efficient thermal energy management across a wide range of sectors leads to continuous exploration of innovative solutions. In this context, phase change materials (PCMs) have emerged as key solutions for thermal energy storage and reuse, offering versatility in addressing contemporary energy challenges. Through this review, we offer a comprehensive critical analysis of the latest developments in PCMs-based technology and their emerging applications within energy systems. First, the conducted investigation highlights the most important drivers stimulating the use of PCMs, namely, the miniaturization of electronic devices, the fluctuating nature of renewable energy sources, and the urge to design smart buildings and textiles. Here, we therefore discuss the integration of PCMs into electronic systems characterized by high heat fluxes, lithium-ion batteries, solar energy systems (including photovoltaic, desalination systems), building materials and textiles to offer wearable solutions for enhanced thermal comfort. Outlining around 100 various cases, PCMs emerge as particularly suitable to ensure optimal operating temperature ranges, to extend lifespan of the devices and ultimately to improve overall system energy efficiency. Beyond potential, challenges such as material leakage, long-term durability, and cost-effectiveness are discussed. By focusing on literature post-2022, the proposed review aims to condense the latest numerical and experimental research findings, spotlight emerging trends, and identify challenges to promote broader and long-term adoption of PCM-based systems. By providing a holistic perspective on PCM applications, we emphasize their potential in achieving sustainable and efficient energy management and provide insights to encourage future cross-disciplinary research and innovation.}
}
@article{PANGGABEAN2025100412,
title = {How do ChatGPT's benefit–risk-coping paradoxes impact higher education in Taiwan and Indonesia?},
journal = {Computers and Education: Artificial Intelligence},
volume = {8},
pages = {100412},
year = {2025},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2025.100412},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X25000529},
author = {Ellis Mardiana Panggabean and Andri Dayarana K. Silalahi},
keywords = {Artificial intelligence, Benefit-coping-risk, chatgpt in education, fsqca, Education, Technology adoption},
abstract = {The integration of ChatGPT into higher education in Taiwan and Indonesia presents both opportunities and challenges. This integration creates a paradox of benefits and risks that must be carefully managed. While previous studies have explored ChatGPT's applications, its complexities in educational contexts remain partially unaddressed. This study bridges that gap by integrating the Unified Theory of Acceptance and Use of Technology (UTAUT) with Protection Motivation Theory (PMT) to examine ChatGPT's role through a benefit–risk–coping mechanism. Data were collected from higher education users in Taiwan and Indonesia. Structural Equation Modeling (SEM) revealed distinct patterns in the two regions. In Taiwan, perceived severity reduces the intention to use ChatGPT, while self-efficacy fosters adoption. In Indonesia, users emphasize response efficacy and performance expectancy as stronger predictors of usage intention. Task efficiency and performance expectancy enhance usage intention in both settings, with Indonesia showing a stronger link between intention and actual use. Fuzzy sets Qualitative Comparative Analysis (fsQCA) further identifies diverse configurations for actual usage and disusage of ChatGPT. Task efficiency and performance expectancy emerge as key usage drivers in both contexts. Disusage in Taiwan primarily stems from task inefficiency, whereas multiple factors—including low self-efficacy—hinder adoption in Indonesia. These findings provide practical insights for higher education institutions, guiding strategies to optimize ChatGPT's benefits, minimize risks, and ensure its responsible use in educational settings across Taiwan and Indonesia.}
}
@incollection{VINTER2025,
title = {Artificial intelligence in GPCR drug discovery: A paradigm shift in computational pharmacology},
booktitle = {Reference Module in Chemistry, Molecular Sciences and Chemical Engineering},
publisher = {Elsevier},
year = {2025},
isbn = {978-0-12-409547-2},
doi = {https://doi.org/10.1016/B978-0-443-29808-0.00047-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780443298080000479},
author = {Adrijana Vinter and Ivan Grgičević},
keywords = {G protein-coupled receptors, Artificial intelligence, Drug discovery, Machine learning, Deep learning, Graph neural networks, Reinforcement learning, AlphaFold2, Biased signaling, Allosteric modulators, Multi-omics integration, Virtual screening, De novo drug design, Precision medicine, Explainable AI (XAI)},
abstract = {This Chapter explores the transformative role of artificial intelligence in GPCR-targeted drug discovery, highlighting how machine learning, deep learning, and reinforcement learning are reshaping ligand screening, structure prediction, and personalized medicine. AI models such as AlphaFold2, graph neural networks, and generative adversarial networks have significantly accelerated hit identification, improved functional selectivity, and enabled allosteric modulator discovery. Integrated with multi-omics data, AI enhances the precision and efficiency of therapeutic development while reducing cost and time. The Chapter also addresses the challenges of data scarcity, model interpretability, and experimental validation, offering potential solutions through explainable AI and hybrid workflows. These advancements position AI not just as a supportive tool but as a central driver in next-generation GPCR pharmacology and precision drug design.}
}
@article{XIAO2024e37238,
title = {RETRACTED: The promises and challenges of AI-based chatbots in language education through the lens of learner emotions},
journal = {Heliyon},
volume = {10},
number = {18},
pages = {e37238},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e37238},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024132690},
author = {Yuehai Xiao and Tianyu Zhang and Jingyi He},
abstract = {This article has been retracted: please see Elsevier policy on article withdrawal (https://www.elsevier.com/about/policies-and-standards/article-withdrawal). This article has been retracted at the request of the Editor-in-Chief. Post-publication, the journal identified references that are irrelevant to the article. The authors were asked to comment upon the presence of these references in their work but were unable to satisfactorily address the reason for the references. Consequently, the editor no longer has confidence in the integrity and the findings of the article and has decided to retract it. The scientific community takes a very strong view on this matter and apologies are offered to readers of the journal that this was not detected during the submission process. The authors disagree with retraction and dispute the grounds for it.}
}
@article{ZAHRA2024122172,
title = {Current advances in imaging spectroscopy and its state-of-the-art applications},
journal = {Expert Systems with Applications},
volume = {238},
pages = {122172},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.122172},
url = {https://www.sciencedirect.com/science/article/pii/S095741742302674X},
author = {Anam Zahra and Rizwan Qureshi and Muhammad Sajjad and Ferhat Sadak and Mehmood Nawaz and Haris Ahmad Khan and Muhammad Uzair},
keywords = {Imaging spectroscopy, Hyperspectral imaging, Image processing, Computer vision, Remote sensing, Deep learning},
abstract = {Imaging spectroscopy integrates traditional computer vision and spectroscopy into a single system and has gained widespread acceptance as a non-destructive scientific instrument for a wide range of applications. The current state of imaging spectroscopy spans diverse applications including but not limited to air-borne and ground-based computer vision systems. This paper presents the current state of research and industrial applications including precision agriculture, material classification, medical science, forensic science, face recognition and document image analysis, environment monitoring, and remote sensing, which can be aided through imaging spectroscopy. In this regard, we further discuss a comprehensive list of applications of imaging spectroscopy, pre-processing techniques, and spectral image acquisition systems. Likewise, publicly available databases and current software tools for spectral data analysis are also documented in this review. This review paper, therefore, could potentially serve as a reference and roadmap for people looking for literature, databases, applications, and tools to undertake additional research in imaging spectroscopy.}
}
@article{LU2025103514,
title = {Integrating language into medical visual recognition and reasoning: A survey},
journal = {Medical Image Analysis},
volume = {102},
pages = {103514},
year = {2025},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2025.103514},
url = {https://www.sciencedirect.com/science/article/pii/S1361841525000623},
author = {Yinbin Lu and Alan Wang},
keywords = {Vision language Model, Medical imaging analysis, Multimodal learning, Visual recognition and reasoning},
abstract = {Vision-Language Models (VLMs) are regarded as efficient paradigms that build a bridge between visual perception and textual interpretation. For medical visual tasks, they can benefit from expert observation and physician knowledge extracted from textual context, thereby improving the visual understanding of models. Motivated by the fact that extensive medical reports are commonly attached to medical imaging, medical VLMs have triggered more and more interest, serving not only as self-supervised learning in the pretraining stage but also as a means to introduce auxiliary information into medical visual perception. To strengthen the understanding of such a promising direction, this survey aims to provide an in-depth exploration and review of medical VLMs for various visual recognition and reasoning tasks. Firstly, we present an introduction to medical VLMs. Then, we provide preliminaries and delve into how to exploit language in medical visual tasks from diverse perspectives. Further, we investigate publicly available VLM datasets and discuss the challenges and future perspectives. We expect that the comprehensive discussion about state-of-the-art medical VLMs will make researchers realize their significant potential.}
}
@article{CASPRINI2024100621,
title = {Untangling the yarn: A contextualization of human resource management to the family firm setting},
journal = {Journal of Family Business Strategy},
volume = {15},
number = {3},
pages = {100621},
year = {2024},
issn = {1877-8585},
doi = {https://doi.org/10.1016/j.jfbs.2024.100621},
url = {https://www.sciencedirect.com/science/article/pii/S1877858524000160},
author = {Elena Casprini and Rocco Palumbo and Alfredo {De Massis}},
keywords = {Family firm, Human resource management, Family resource management, Nonfamily resource management, People management},
abstract = {Despite the efforts to contextualize human resource management to family firms, scientific literature addressing this study domain suffers from limited systematization. The article arranges an integrative framework to make sense of the challenges faced by family firms in designing and implementing human resource management practices. Bibliographic coupling was run on an intellectual core of 69 papers to illuminate dominant research streams. Besides, co-citation was executed to determine the conceptual roots nurturing recent scholarly advancements. A dance between formality and informality of human resource management practices characterizes extant research, calling for developments to understand how family firms can deal with it.}
}
@article{2024I,
title = {Abstracts for SAR/RCMI PolyU International Research Conference},
journal = {Journal of Integrative Medicine},
volume = {22},
number = {3},
pages = {I-LXXVI},
year = {2024},
issn = {2095-4964},
doi = {https://doi.org/10.1016/S2095-4964(24)00328-5},
url = {https://www.sciencedirect.com/science/article/pii/S2095496424003285}
}
@article{PITAKASO2025102011,
title = {AI-driven cultural urbanism: A data-integrated model for learning city development in emerging heritage contexts},
journal = {Social Sciences & Humanities Open},
volume = {12},
pages = {102011},
year = {2025},
issn = {2590-2911},
doi = {https://doi.org/10.1016/j.ssaho.2025.102011},
url = {https://www.sciencedirect.com/science/article/pii/S2590291125007399},
author = {Rapeepan Pitakaso and Surajet Khonjan and Thanatkij Srichok and Natthapong Nanthasamroeng and Paweena Khampukka and Arunrat Sawettham and Sairoong Dinkoksung and Chawapot Supasarn and Kanya Jungvimutipan and Yong Boonarree and Ganokgarn Jirasirilerd and Pornpimol Mongkhonngam},
keywords = {Learning cities, Generative AI, Cultural heritage Design, Persona-based planning, Participatory urbanism},
abstract = {This study introduces an AI-enhanced framework for designing culturally rooted Learning Cities, demonstrated through Warinchamrap, Thailand—a heritage-rich secondary city overlooked by conventional urban development. Addressing gaps in smart city models that neglect symbolic landscapes, community narratives, and intergenerational learning, this research merges AI modeling with participatory cultural mapping and spatial analytics to create eight location-specific learning nodes.The methodology comprised five phases: (1) integrating multi-source data (archival, oral history, geospatial, social media); (2) applying AI clustering and natural language processing to identify user personas and spatial patterns; (3) employing generative AI (GPT-4, diffusion models) for culturally appropriate design concepts; (4) evaluating designs via a Design Suitability Score (DSS) framework combining AI metrics and stakeholder validation; and (5) refining architectural designs based on community feedback.Initial AI concepts achieved DSS scores averaging 0.866. After prompt refinement, version 2 designs improved to 0.908. Final architectural designs, informed by AI outputs, maintained strong community alignment with a DSS of 0.893. Projects like Songsarn x Rails of Memory and Warin Light Avenue successfully integrated spatial storytelling, cultural heritage, and adaptive learning design. Findings demonstrate that culturally embedded AI can serve as an effective co-designer for inclusive, memory-driven urban learning environments. This research provides a replicable framework for Learning Cities discourse, synthesizing generative AI, local heritage, and spatial intelligence. It offers valuable insights for developing culturally sustainable smart cities, adaptive tourism, and community-driven urban design, establishing AI as a responsive tool when anchored in cultural semantics and participatory engagement.}
}
@article{ZHANG2025252,
title = {Digital twin-driven staged error prediction and compensation framework for the whole process of robotic machining},
journal = {Journal of Manufacturing Systems},
volume = {83},
pages = {252-283},
year = {2025},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2025.09.009},
url = {https://www.sciencedirect.com/science/article/pii/S0278612525002365},
author = {Teng Zhang and Fangyu Peng and Zhao Yang and Xiaowei Tang and Jiangmiao Yuan and Rong Yan},
keywords = {Robotic machining, Error prediction and compensation, Staged strategies, Digital twins},
abstract = {Robotic machining has become another important machining paradigm after CNC machine tools. However, robot error has always been an important constraint in its progress towards high quality demand scenarios due to characteristics such as weak rigidity and pose dependence. Numerous scholars have carried out rich work around errors in robotic machining systems, and these studies have achieved excellent results in robot localization, trajectory continuous motion, and machining operations. However, due to the complexity of the robot machining system, the robot error has differentiated performance at different stages, and it is difficult to guarantee the global accuracy of the robot by focusing on and controlling a certain kind of error in a discrete manner. For this reason, a digital twin-driven staged error prediction and compensation framework for the whole robot machining process is constructed. In this framework, the whole process of robot machining is divided into three stages with significant differences: point planning, trajectory planning and material removal. And the error prediction function block in each stage is constructed for the error characteristics (distribution skew, error step, spatial-temporal coupling). For error compensation, a staged error compensation strategy is constructed from three aspects: offline point position, robot body and external three-axis platform, respectively. The constructed system was case-validated in the robotic machining of curved parts. All stages of the error prediction models show high prediction accuracy, and the excellent performance of the staged prediction models is verified by comparing with the classical prediction models. For the error compensation, the designed system is utilized to ensure that the robotic machining system provides a double guarantee on the robot end and the machining quality, the point position absolute error is controlled at 0.109 mm, the orientation error is controlled at 0.028°, the trajectory position error is controlled at 0.067 mm, the orientation error is controlled at 0.031°, and the final part machining error is controlled at 0.036 mm, which is almost approximates the repeatable positioning accuracy of the robot. The proposed framework realizes the system-level sensing and control of the robot machining system error, and provides a unified system framework for the subsequent research of related unit methods, which is conducive to promoting the development of robot machining to high-quality requirement scenarios.}
}
@incollection{SAFARI2025,
title = {Blockchain innovations for transparent energy transactions},
booktitle = {Reference Module in Materials Science and Materials Engineering},
publisher = {Elsevier},
year = {2025},
isbn = {978-0-12-803581-8},
doi = {https://doi.org/10.1016/B978-0-443-29210-1.00038-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780443292101000388},
author = {Ashkan Safari and Arman Oshnoei and Ahsan Nadeem and Frede Blaabjerg},
keywords = {Artificial intelligence, Blockchain technology, Smart grids, Transactive energy systems},
abstract = {Blockchain is among the key technologies in transforming trade by creating a secure, transparent, and decentralized platform for transactions. It eliminates intermediaries, reducing costs and increasing efficiency in supply chains, cross-border payments, and contract execution in transactive energy system (TES). By using distributed ledger systems, blockchain facilitates peer-to-peer (P2P) energy transactions, allowing prosumers to buy/sell excess energy, such as from renewable energy sources (RES), without intermediaries. To this end, this article presents a detailed explanation and different frameworks for blockchain-based TES markets in a complete perspective. Firstly, the concept of TES, its integration with distributed energy resource (DER), and different considered objective functions are presented. Then, the fundamentals of blockchain, its different types, algorithms, and each type's integration in TES are taken into account. Following this concept, the smart contracts structure, algorithms, as well as different hash function, their mathematical forms, and overall processes are exemplified. Therefore, different transaction logs of direct producer-consumer and marketplace are considered to provide the context for potential applications of artificial intelligence (AI). An extensive literature review of AI is conducted, and three example AI integrations in blockchain-based TES are manifested, as a selection for security-focused (federated learning [FL], dynamic-dependence [Deep Reinforcement Learning (DRL)], and data-dependence [long-short term memory (LSTM)]) models. Therefore, a discussion is provided upon the long-term sustainability goals and also TES alliance with sustainability standards. Finally, challenges and future works on security and user privacy aspects of blockchain-based TES are presented. The blockchain and AI integration in TES enhance grid resilience, promotes adoption, and empowers consumers, making TES markets more accessible and scalable for a sustainable energy future.}
}
@article{HERBOSCH2024105941,
title = {Fraud by generative AI chatbots: On the thin line between deception and negligence},
journal = {Computer Law & Security Review},
volume = {52},
pages = {105941},
year = {2024},
issn = {2212-473X},
doi = {https://doi.org/10.1016/j.clsr.2024.105941},
url = {https://www.sciencedirect.com/science/article/pii/S0267364924000086},
author = {Maarten Herbosch},
keywords = {Artificial intelligence, Contract law, Fraud, Law of obligations, Vice of consent},
abstract = {The use of generative AI systems is on the rise. As a result, we are increasingly often conversing with AI chatbots rather than with fellow humans. This increasing use of AI systems leads to legal challenges as well, particularly when the chatbot provides incorrect information. In this article, we study whether someone who decides to contract on the basis of incorrect information provided by a generative AI chatbot might invoke the fraud regime to annul the resulting contract in various legal systems. During this analysis, it becomes clear that some of the requirements that are currently being put forward from a public law perspective, such as in the European AI Act, may also naturally arise from existing private law figures. In the same vein, this analysis highlights the interesting intradisciplinary feedback between instruments of public law and other legal domains.}
}
@article{WEI2026103049,
title = {Cooperative supervision of livestreaming e-commerce based on stochastic evolutionary game and overconfidence},
journal = {Technology in Society},
volume = {84},
pages = {103049},
year = {2026},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2025.103049},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X25002398},
author = {Xiaochao Wei and Qiping She},
keywords = {Livestreaming e-commerce, Cooperative supervision, Streamer type, Overconfidence, Evolutionary game},
abstract = {The rapid development of livestreaming e-commerce has been accompanied by an increasing number of misleading marketing behaviors (MIBs) that require adequate regulations. To reveal the impact of irrational behavior (overconfidence) and to explore effective supervision strategies tailored to different types of streamers. We have classified streamers into brand-affiliated streamers and professional streamers (including internet celebrity streamers and ordinary streamers), then four types overconfidence are identified and integrated into a stochastic evolutionary game framework to investigate the regulatory effectiveness. The findings indicate that platform overconfidence positively affects supervision, while overconfidence among streamers and consumers has the opposite effect. For brand-affiliate streamers, the reputation mechanism exerts the most significant regulatory influence and should be heightened; additionally, enhancing platform penalties proves effective in cases of streamer overconfidence, whereas reducing supervision costs works better in other scenarios. Regarding professional streamers, a combination of platform penalties and incentive mechanisms leads to more stable and effective regulatory outcomes, especially in cases of streamer or consumer overconfidence. Furthermore, for internet celebrity streamers, the reputation mechanism serves as a beneficial supplement; for ordinary streamers, reducing regulatory costs proves to be more effective. Therefore, this study provides insights for classified and tiered regulation policy formulation regarding livestreaming e-commerce and provides a new perspective for supervision research by integrating overconfidence and evolutionary games.}
}
@article{CALISKAN20234895,
title = {Metadata integrity in bioinformatics: Bridging the gap between data and knowledge},
journal = {Computational and Structural Biotechnology Journal},
volume = {21},
pages = {4895-4913},
year = {2023},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2023.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S2001037023003616},
author = {Aylin Caliskan and Seema Dangwal and Thomas Dandekar},
keywords = {Meta-data, Error, Annotation, Error-transfer, Wrong labelling, Patient data, Control group, Tools overview},
abstract = {In the fast-evolving landscape of biomedical research, the emergence of big data has presented researchers with extraordinary opportunities to explore biological complexities. In biomedical research, big data imply also a big responsibility. This is not only due to genomics data being sensitive information but also due to genomics data being shared and re-analysed among the scientific community. This saves valuable resources and can even help to find new insights in silico. To fully use these opportunities, detailed and correct metadata are imperative. This includes not only the availability of metadata but also their correctness. Metadata integrity serves as a fundamental determinant of research credibility, supporting the reliability and reproducibility of data-driven findings. Ensuring metadata availability, curation, and accuracy are therefore essential for bioinformatic research. Not only must metadata be readily available, but they must also be meticulously curated and ideally error-free. Motivated by an accidental discovery of a critical metadata error in patient data published in two high-impact journals, we aim to raise awareness for the need of correct, complete, and curated metadata. We describe how the metadata error was found, addressed, and present examples for metadata-related challenges in omics research, along with supporting measures, including tools for checking metadata and software to facilitate various steps from data analysis to published research.}
}
@article{FRISTON2024105500,
title = {Federated inference and belief sharing},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {156},
pages = {105500},
year = {2024},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2023.105500},
url = {https://www.sciencedirect.com/science/article/pii/S0149763423004694},
author = {Karl J. Friston and Thomas Parr and Conor Heins and Axel Constant and Daniel Friedman and Takuya Isomura and Chris Fields and Tim Verbelen and Maxwell Ramstead and John Clippinger and Christopher D. Frith},
keywords = {Active inference, Distributed cognition, Federated learning, Structure learning, Message passing},
abstract = {This paper concerns the distributed intelligence or federated inference that emerges under belief-sharing among agents who share a common world—and world model. Imagine, for example, several animals keeping a lookout for predators. Their collective surveillance rests upon being able to communicate their beliefs—about what they see—among themselves. But, how is this possible? Here, we show how all the necessary components arise from minimising free energy. We use numerical studies to simulate the generation, acquisition and emergence of language in synthetic agents. Specifically, we consider inference, learning and selection as minimising the variational free energy of posterior (i.e., Bayesian) beliefs about the states, parameters and structure of generative models, respectively. The common theme—that attends these optimisation processes—is the selection of actions that minimise expected free energy, leading to active inference, learning and model selection (a.k.a., structure learning). We first illustrate the role of communication in resolving uncertainty about the latent states of a partially observed world, on which agents have complementary perspectives. We then consider the acquisition of the requisite language—entailed by a likelihood mapping from an agent's beliefs to their overt expression (e.g., speech)—showing that language can be transmitted across generations by active learning. Finally, we show that language is an emergent property of free energy minimisation, when agents operate within the same econiche. We conclude with a discussion of various perspectives on these phenomena; ranging from cultural niche construction, through federated learning, to the emergence of complexity in ensembles of self-organising systems.}
}
@article{MANTELERO2024106020,
title = {The Fundamental Rights Impact Assessment (FRIA) in the AI Act: Roots, legal obligations and key elements for a model template},
journal = {Computer Law & Security Review},
volume = {54},
pages = {106020},
year = {2024},
issn = {2212-473X},
doi = {https://doi.org/10.1016/j.clsr.2024.106020},
url = {https://www.sciencedirect.com/science/article/pii/S0267364924000864},
author = {Alessandro Mantelero},
keywords = {AI Act, Fundamental rights impact assessment, FRIA, Fundamental Rights, AI},
abstract = {What is the context which gave rise to the obligation to carry out a Fundamental Rights Impact Assessment (FRIA) in the AI Act? How has assessment of the impact on fundamental rights been framed by the EU legislator in the AI Act? What methodological criteria should be followed in developing the FRIA? These are the three main research questions that this article aims to address, through both legal analysis of the relevant provisions of the AI Act and discussion of various possible models for assessment of the impact of AI on fundamental rights. The overall objective of this article is to fill existing gaps in the theoretical and methodological elaboration of the FRIA, as outlined in the AI Act. In order to facilitate the future work of EU and national bodies and AI operators in placing this key tool for human-centric and trustworthy AI at the heart of the EU approach to AI design and development, this article outlines the main building blocks of a model template for the FRIA. While this proposal is consistent with the rationale and scope of the AI Act, it is also applicable beyond the cases listed in Article 27 and can serve as a blueprint for other national and international regulatory initiatives to ensure that AI is fully consistent with human rights.}
}
@article{PENG2024199,
title = {A narrative review of Environmentally Oriented Anti-consumption: Definitions, dimensions, and research framework},
journal = {Sustainable Production and Consumption},
volume = {51},
pages = {199-221},
year = {2024},
issn = {2352-5509},
doi = {https://doi.org/10.1016/j.spc.2024.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S2352550924001933},
author = {Feiyan Peng and Anhua Long and Juan Chen and Khloe Qi Kang},
keywords = {Anti-consumption, Environmental concerns, Environmentally Oriented Anti-consumption, Sustainable development, Policy enlightenment},
abstract = {In the context of political uncertainty, environmental degradation, and resource scarcity, significant changes in individual consumption attitudes underscore the necessity of sustainability and anti-consumption research. Environmentally-oriented anti-consumption (EOA) represents a pivotal research direction that integrates these elements. Utilizing the PRISMA method, we conducted a comprehensive analysis of 428 articles. Our findings indicate that while qualitative methods have traditionally been favored, quantitative research is rapidly increasing. However, the dimensions, measurements, and frameworks employed in quantitative research remain fragmented, signaling a need for further refinement in EOA studies. To advance the theoretical framework of EOA, we rigorously selected and systematically analyzed 36 articles. Following identification, refinement, and expert validation, we proposed a comprehensive taxonomy categorizing EOA into seven major types, each with various sub-dimensions and measurement items. Furthermore, we developed a framework to measure the antecedents and consequences of EOA, incorporating motivational explanatory mechanisms. Our research provides a more precise definition and scope of EOA, thereby enhancing academic understanding. It offers novel tools for businesses and policymakers to implement sustainable practices, positioning on target groups through classification and dimensional measurement. This study aligns policies, marketing, and consumer behavior with sustainability goals, promoting societal development and addressing challenges in evolving social and environmental contexts.}
}
@article{VELDHUIS2025100708,
title = {Critical Artificial Intelligence literacy: A scoping review and framework synthesis},
journal = {International Journal of Child-Computer Interaction},
volume = {43},
pages = {100708},
year = {2025},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2024.100708},
url = {https://www.sciencedirect.com/science/article/pii/S2212868924000771},
author = {Annemiek Veldhuis and Priscilla Y. Lo and Sadhbh Kenny and Alissa N. Antle},
keywords = {Artificial intelligence, Critical literacy, AI ethics, AI literacy, Computational empowerment, Literature review},
abstract = {The proliferation of Artificial Intelligence (AI) in everyday life raises concerns for children, other marginalized groups, and the general public. As new AI implementations continue to emerge, it is crucial to enable children to engage critically with AI. Critical literacy objectives and practices can encourage children to question, critique, and transform the social, political, cultural, and ethical implications of AI. As an initial step towards critical AI education, we conducted a 10-year scoping review to identify publications reporting on activities that engage children, between the ages of 5 and 18, to address the critical implications of AI. Our review identifies a wide range of participants, content, and pedagogical approaches. Through framework synthesis guided by an established critical literacy model, we examine the critical literacy learning objectives embedded in the reported activities and propose a critical AI literacy framework. This paper outlines future opportunities for critical AI literacies in the field of child–computer interaction including inspiring new learning activities, encouraging inclusive perspectives, and supporting pragmatic curriculum integration.}
}
@incollection{BOHR202025,
title = {Chapter 2 - The rise of artificial intelligence in healthcare applications},
editor = {Adam Bohr and Kaveh Memarzadeh},
booktitle = {Artificial Intelligence in Healthcare},
publisher = {Academic Press},
pages = {25-60},
year = {2020},
isbn = {978-0-12-818438-7},
doi = {https://doi.org/10.1016/B978-0-12-818438-7.00002-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128184387000022},
author = {Adam Bohr and Kaveh Memarzadeh},
keywords = {Artificial intelligence, healthcare applications, machine learning, precision medicine, ambient assisted living, natural language programming, machine vision},
abstract = {Big data and machine learning are having an impact on most aspects of modern life, from entertainment, commerce, and healthcare. Netflix knows which films and series people prefer to watch, Amazon knows which items people like to buy when and where, and Google knows which symptoms and conditions people are searching for. All this data can be used for very detailed personal profiling, which may be of great value for behavioral understanding and targeting but also has potential for predicting healthcare trends. There is great optimism that the application of artificial intelligence (AI) can provide substantial improvements in all areas of healthcare from diagnostics to treatment. It is generally believed that AI tools will facilitate and enhance human work and not replace the work of physicians and other healthcare staff as such. AI is ready to support healthcare personnel with a variety of tasks from administrative workflow to clinical documentation and patient outreach as well as specialized support such as in image analysis, medical device automation, and patient monitoring. In this chapter, some of the major applications of AI in healthcare will be discussed covering both the applications that are directly associated with healthcare and those in the healthcare value chain such as drug development and ambient assisted living.}
}
@article{XU2024123923,
title = {Collaborative optimization of multi-energy multi-microgrid system: A hierarchical trust-region multi-agent reinforcement learning approach},
journal = {Applied Energy},
volume = {375},
pages = {123923},
year = {2024},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2024.123923},
url = {https://www.sciencedirect.com/science/article/pii/S0306261924013060},
author = {Xuesong Xu and Kai Xu and Ziyang Zeng and Jiale Tang and Yuanxing He and Guangze Shi and Tao Zhang},
keywords = {Multi-microgrid system, Integrated multi-energy network, Collaboration optimization, Flexible retraining mechanism, Hierarchical multi-agent reinforcement learning},
abstract = {In the context of the expanding diversity of energy demands, an increasing number of heterogeneous Multi-energy Microgrids (MEMGs) are engaging in the collaborative framework of the Multi-energy Multi-microgrid System (MEMMG). However, following this trend, the existing centralized Integrated Energy Management System (IEMS) control strategy is unreliable for future energy systems, characterized by more complex optimization control and a flexible system structure. This paper introduces a hierarchical Multi-agent Deep Reinforcement Learning (HMADRL) approach for distributed IEMS in MEMMG. Firstly, by employing a hierarchical approach, this method simplifies control complexity by segmenting the overarching control challenge into tasks of collaborative planning and action control, which are distributed across varied multi-agent scenes. Considering both macro and microeconomic factors, alongside carbon emissions, the optimal operation of MEMMG is realized through a bottom-up edge multi-agent control approach, in contrast to traditional top-down centralized methods. Secondly, in the phase of the inter-MEMG collaborative strategy, the Centralized Training Decentralized Execution (CTDE) framework is adopted to overcome the problems of unstable training environments and large-scale agent training, and each heterogeneous microgrid can develop local strategies independently with the assurance that their internal data will not be overly exposed. Thirdly, within each MEMG, the Trust-Region (TR) model is introduced for multi-agent action control, adeptly addressing the effects of mutual exclusion in decision-making time series. Simultaneously, an initialized Hot Experience Pool (HEP) is implemented, efficiently reducing exploration in complex, high-dimensional spaces. Finally, the off-time agent model is integrated into the HMADRL environment and undergoes secondary training based on real interactions, thereby deriving the optimal energy management policy. The proposed method markedly reduces reliance on exact physical modeling systems. Comparative simulations validate the proposed control scheme’s efficacy.}
}
@article{SMIMOU2024351,
title = {Commodities and Policy Uncertainty Channel(s)},
journal = {International Review of Economics & Finance},
volume = {92},
pages = {351-379},
year = {2024},
issn = {1059-0560},
doi = {https://doi.org/10.1016/j.iref.2024.01.065},
url = {https://www.sciencedirect.com/science/article/pii/S1059056024000650},
author = {K. Smimou and D. Bosch and G. Filbeck},
keywords = {Hedgers, Speculators, Commodity stock markets, Commodity futures, Policy uncertainty, Financialization of commodities},
abstract = {Based on a proposed linked theoretical model, this study dissects the contributory role of policy uncertainty on commodity futures contracts and pertinent commodity equity sectors. Within an economically connected framework, we elucidate the dynamic relationship between these groups of assets while allowing for policy uncertainty shock. Findings show that commodity hedgers altered trading positions in metals in response to a high policy uncertainty shock before 2004. In contrast, speculators account for that shock after that date purely via crude oil. Both monetary policy and regulatory uncertainties influence the pricing dynamics of metals and energy commodities. Given the inextricable commodity–stock relationship, we offer evidence to support the triple effect of economic policy uncertainty on the intensiveness of financialization of commodities and commodity stock returns through (1) change of institutional holdings, (2) managers’ alteration of CAPEX investment of commodity firms, and (3) the interactive causality channel between commodity futures and commodity stocks.}
}
@article{YAN2025126743,
title = {Representing similarities of map projections: An approach to approximate integrations and dimensionality reductions},
journal = {Expert Systems with Applications},
volume = {274},
pages = {126743},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.126743},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425003653},
author = {Jin Yan and Jun Zhang and Tiansheng Xu and Jing Gao and Ni Li and Guanghong Gong},
keywords = {Map projection, Distance matrix, Distortion, Integrals calculation, Approximation, Dimensionality reduction},
abstract = {This study addresses key limitations in structured or hierarchical classification and pairwise comparison methods for analyzing map projections, which often fail to capture the intricate relationships among them. It introduces an innovative approach that automates and simplifies the approximation of complex results from an improved integration metric for measuring (dis)similarities between projections. This enhances the understanding of over 340 map projections comprehensively. The study combines formula-based and image-based methodologies within a hybrid sampling scheme, effectively handling complex integrations, particularly for projections with intricate formulas. Using NASA’s G.Projector mapping software, an 87 GB image dataset is generated for 175 map projections, while an additional 170 projections are processed through a formula-based approach. The approximate integral calculations closely align with theoretical values, with an acceptable error margin of a few thousandths. To visually represent map projections in two-dimensional space, multiple dimensionality reduction techniques are employed, incorporating features such as distortions. The resulting quantitative metrics demonstrate that global, local, and category-based features are reasonably preserved. A clear and intuitive visual representation simplifies the complexity of map projection relationships, offering valuable insights. Additionally, an interactive web application prototype is developed to showcase the relationships among map projections. To the best of our knowledge, this research is the first to comprehensively evaluate such a large number of map projections using automated calculations combined with dimensionality reduction and visualization techniques. This methodology represents a significant advancement in cartography, providing a robust framework for comparing and analyzing map projections in practice.}
}
@article{ABADIE2024304,
title = {Impact of carbon offset perceptions on greenwashing: Revealing intentions and strategies through an experimental approach},
journal = {Industrial Marketing Management},
volume = {117},
pages = {304-320},
year = {2024},
issn = {0019-8501},
doi = {https://doi.org/10.1016/j.indmarman.2024.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0019850124000014},
author = {Amelie Abadie and Soumyadeb Chowdhury and Sachin Kumar Mangla and Shaily Malik},
keywords = {Carbon offset, Greenwashing, Agency theory, Revealed preferences, Experiment},
abstract = {Organizations operating in the Business-to-Business (B2B) ecosystem across the globe are committed to net zero initiatives to achieve sustainability across business processes. In this context, carbon carbon credits have emerged as a carbon offsetting mechanism to help organizations invest in low-carbon initiatives. However, existing studies are yet to examine whether carbon offsetting practices will influence the sustainability behavior of B2B organizations and whether it could lead to greenwashing propensity. In this vein, we adopt agency and revealed pereferences theories to conduct two experiments with B2B small and medium-sized enterprises (SMEs) managers operating in the UK process intensive sectors to reveal that affordability of carbon credits can motivate managers to engage in sustainable attitudes and practices. We also found that organizations willing to buy carbon credits at high price to are likely to engage in high greenwashing propensity. Considering these novel findings, we provide recommendations that will help organizations to become more responsible in their carbon offesteing investments, and for policy makers to adopt stringent assessment of such investments and carbon disclosures made by firms.}
}
@article{ABUALIGAH2025100646,
title = {A control-driven transition strategy for enhanced multi-level threshold image segmentation optimization},
journal = {Egyptian Informatics Journal},
volume = {30},
pages = {100646},
year = {2025},
issn = {1110-8665},
doi = {https://doi.org/10.1016/j.eij.2025.100646},
url = {https://www.sciencedirect.com/science/article/pii/S1110866525000398},
author = {Laith Abualigah and Mohammad H. Almomani and Saleh Ali Alomari and Raed Abu Zitar and Vaclav Snasel and Kashif Saleem and Aseel Smerat and Absalom E. Ezugwu},
keywords = {Flood Algorithm, Non-Monopolize search, Multi-level threshold, Image segmentation, Transition mechanism},
abstract = {This work proposes an image segmentation approach based on a multi-threshold segmentation method and the enhanced Flood Algorithm combined with the Non-Monopolize search (named Improved IFLANO). The introduced approach, depending on IFLANO, offers much better segmentation quality for various images. Based on the existing structure, two different types of optimization techniques are added within IFLANO to enhance the update dynamics during optimization. The random strategy used in the Aquila optimization procedure enhances the performance of FLA, and a self-transition adaptation enhances the exploration ability of the image analysis. IFLANO framework is implemented for multi-level threshold image segmentation wherein the evaluation metric is Kapur’s entropy-based between-class variance. Benchmarking studies against standard test images show that IFLANO works not only faster but also yields better, more stable outcomes in image segmentations within similar time frames. IFLANO is shown to put any solution a step forward in its search for more accurate alternatives than any of the considered techniques by getting 96% improvement. We also find that the proposed method got better results in solving large medical clustering applications.}
}
@article{FOROUZANFAR2025177690,
title = {Significance of NMDA receptor-targeting compounds in neuropsychological disorders: An in-depth review},
journal = {European Journal of Pharmacology},
volume = {999},
pages = {177690},
year = {2025},
issn = {0014-2999},
doi = {https://doi.org/10.1016/j.ejphar.2025.177690},
url = {https://www.sciencedirect.com/science/article/pii/S0014299925004443},
author = {Fatemeh Forouzanfar and Amir Mahmoud Ahmadzadeh and Ali Mohammad Pourbagher-Shahri and Ali Gorji},
keywords = {Excitotoxicity, Glutamate receptors, Neurological diseases, Brain, Synaptic plasticity},
abstract = {N-methyl-D-aspartate receptors (NMDARs), a subclass of glutamate-gated ion channels, play an integral role in the maintenance of synaptic plasticity and excitation-inhibition balance within the central nervous system (CNS). Any irregularities in NMDAR functions, whether hypo-activation or over-activation, can destabilize neural networks and impair CNS function. Several decades of experimental and clinical investigations have demonstrated that NMDAR dysfunction is implicated in the pathophysiology of various neurological disorders. Despite designing a long list of compounds that differentially modulate NMDARs, success in developing drugs that can selectively and effectively regulate various NMDAR subtypes while showing encouraging efficacy in clinical settings remains limited. A better understanding of the basic mechanism of NMDAR function, particularly its selective regulation in pathological conditions, could aid in designing effective drugs for the treatment of neurological conditions. Here, we reviewed the experimental and clinical investigations that studied the effects of available NMDAR modulators in various neurological disorders and weighed up the pros and cons of the use of these substances on the improvement of functional outcomes of these disorders. Despite numerous efforts to develop NMDAR modulatory drugs that did not produce the desired outcomes, NMDARs remain a significant target for advancing novel drugs to treat neurological disorders. This article reviews the complexity of NMDAR signaling dysfunction in different neurological diseases, the efforts taken to examine designed compounds targeting specific subtypes of NMDARs, including challenges associated with using these substances, and the potential enhancements in drug discovery for NMDAR modulatory compounds by innovative technologies.}
}
@article{DEMARTINO2025107678,
title = {Classification and challenges of non-functional requirements in ML-enabled systems: A systematic literature review},
journal = {Information and Software Technology},
volume = {181},
pages = {107678},
year = {2025},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2025.107678},
url = {https://www.sciencedirect.com/science/article/pii/S0950584925000175},
author = {Vincenzo {De Martino} and Fabio Palomba},
keywords = {Software engineering for artificial intelligence, Non-functional requirements, Systematic literature reviews},
abstract = {Context:
Machine learning (ML) is nowadays so pervasive and diffused that virtually no application can avoid its use. Nonetheless, its enormous potential is often tempered by the need to manage non-functional requirements (NFRs) and navigate pressing, contrasting trade-offs.
Objective:
In this respect, we notice a lack of systematic synthesis of challenges explicitly tied to achieving and managing NFRs in ML-enabled systems. Such a synthesis may not only provide a comprehensive summary of the state of the art but also drive further research on the analysis, management, and optimization of NFRS of ML-enabled systems.
Method:
In this paper, we propose a systematic literature review targeting two key aspects such as (1) the classification of the NFRs investigated so far, and (2) the challenges associated with achieving and managing NFRs in ML-enabled systems during model development Through the combination of well-established guidelines for conducting systematic literature reviews and additional search criteria, we survey a total amount of 130 research articles.
Results:
Our findings report that current research identified 31 different NFRs, which can be grouped into six main classes. We also compiled a catalog of 26 software engineering challenges, emphasizing the need for further research to systematically address, prioritize, and balance NFRs in ML-enabled systems.
Conclusion:
We conclude our work by distilling implications and a future outlook on the topic.}
}
@article{PESQUEIRA2024110655,
title = {Exploring the impact of EU tendering operations on future AI governance and standards in pharmaceuticals},
journal = {Computers & Industrial Engineering},
volume = {198},
pages = {110655},
year = {2024},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2024.110655},
url = {https://www.sciencedirect.com/science/article/pii/S0360835224007770},
author = {Antonio Pesqueira and Andreia {de Bem Machado} and Sama Bolog and Rúben Pereira and Maria José Sousa},
keywords = {Artificial Intelligence (AI), Pharmaceutical Industry, Tender Management (TM), Governance, Ethics, Operational Efficiency},
abstract = {This research examines the incorporation of artificial intelligence (AI) into the domain of tender management (TM) within the pharmaceutical industry, with a particular emphasis on operational efficiency, governance, and compliance with European regulatory standards. A comparative analysis of four companies—two that have adopted AI and two that have not—reveals significant discrepancies in the management of TM processes between AI-driven and traditional companies. The study employs the Delphi method to ascertain expert consensus on eight critical areas of AI governance, including data privacy, transparency, and ethical AI use. The findings indicate that companies integrating AI demonstrate enhanced decision-making capabilities, accelerated processing times, and enhanced stakeholder engagement. However, they also encounter challenges pertaining to ethical governance and regulatory compliance. The research highlights the necessity of aligning the adoption of AI with the latest European directives, such as the AI Act and General Data Protection Regulation (GDPR), to ensure both operational efficiency and adherence to ethical standards. The broader implications of the study underscore the necessity for pharmaceutical companies to develop robust governance frameworks, prioritize ethical considerations, and maintain regulatory compliance to fully leverage the potential of AI. Additionally, the study contributes to the ongoing scholarly discourse by providing empirical evidence on the interplay between AI, ethics, and governance, thereby encouraging further interdisciplinary research. This work emphasizes the critical role of strategic AI adoption in maintaining competitive advantage while safeguarding societal trust and adhering to legal requirements.}
}
@article{OYELADE2025127455,
title = {SMAR + NIE IdeaGen: A knowledge graph based node importance estimation with analogical reasoning on large language model for idea generation},
journal = {Expert Systems with Applications},
volume = {279},
pages = {127455},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.127455},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425010772},
author = {Olaide N. Oyelade and Hui Wang and Karen Rafferty},
keywords = {Knowledge graphs (KGs), Large language model (LLMs), Idea generation, Novelty, Analogical reasoning, Node importance estimation, Natural language processing (NLP), Isomorphic subgraphs},
abstract = {Idea generation describes a creative process involving reasoning over some knowledge to derive new information. Traditional approaches such as mind-map and brainstorming are limited and often fail due to lack of quality ideas and ineffective methods. The reasoning capability of large language models (LLMs) have been investigated for ideation tasks and have reported interesting performance. However, these models suffer from limited logical reasoning capability which hinders the use of structural and factual real-world knowledge in discovery of latent insight and predict possible outcome when applied to ideation. In addition, the possibility of LLMs regurgitating knowledge learnt from datasets might adversely impact the degree of novel ideas the models can generate. In this paper, a two-stage logical reasoning approach is applied to initiate the search for candidate idea pathways based on the knowledge graphs (KGs) to address the problem of reasoning, domain-specificity and novelty. The divergence stage this reasoning explores utilizes a new node importance estimation (NIE) technique over KGs to discover latent connections supporting idea generation. In the convergence stage of this reasoning, subgraph matching using analogical reasoning (SMAR) is applied to find matching patterns to describe a new idea. The use of SMAR + NIE and KGs helps to achieve an improvement in reasoning over KGs before transferring such reasoning to LLMs for translation of idea into natural language. To evaluate the degree of novelty of ideas generated, a relevance-to-novelty scoring metrics is proposed based on multiple premise entailment (MPE). We combined this metric with other popular metrics to evaluate the performance of SMAR + NIE on benchmark datasets, and as well on the quality of ideas generated. Findings from the study showed that this approach demonstrates competitive performance with mainstream LLMs in idea generation tasks.}
}
@article{DEWITTE2024106019,
title = {Better alone than in bad company: Addressing the risks of companion chatbots through data protection by design},
journal = {Computer Law & Security Review},
volume = {54},
pages = {106019},
year = {2024},
issn = {2212-473X},
doi = {https://doi.org/10.1016/j.clsr.2024.106019},
url = {https://www.sciencedirect.com/science/article/pii/S0267364924000852},
author = {Pierre Dewitte},
keywords = {Privacy, Data protection, GDPR, AI Act, Data protection by design, Data protection impact assessments, Companion chatbots, Enforcement},
abstract = {Recent years have seen a surge in the development and use of companion chatbots, conversational agents specifically designed to act as virtual friends, romantic partners, life coaches or even therapists. Yet, these tools raise many concerns, especially when their target audience is comprised of vulnerable individuals. While the recently adopted AI Act is expected to address some of these concerns, both compliance and enforcement are bound to take time. Since the development of companion chatbots involves the processing of personal data at nearly every step of the process, from training to fine-tuning to deployment, this paper argues that the General Data Protection Regulation (“GDPR”), and data protection by design more specifically, already provides a solid ground for regulators and courts to force controllers to mitigate these risks. In doing so, it sheds light on the broad material scope of Articles 24(1) and 25(1) GDPR, highlights the role of these provisions as proxies to Fundamental Rights Impact Assessments (“FRIAs”), and peels off the many layers of personal data processing involved in the companion chatbots supply chain. That reasoning served as the basis for a complaint lodged with the Belgian data protection authority, the full text and supporting evidence of which are provided as supplementary materials.}
}
@article{WAQAS2024123893,
title = {Exploring Multiple Instance Learning (MIL): A brief survey},
journal = {Expert Systems with Applications},
volume = {250},
pages = {123893},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.123893},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424007590},
author = {Muhammad Waqas and Syed Umaid Ahmed and Muhammad Atif Tahir and Jia Wu and Rizwan Qureshi},
keywords = {Multiple Instance Learning (MIL), Multi-Instance Learning(MIL), SUpervised MIL, Unsupervised MIL, Bag and Instance Classification, Review, MIL Applications},
abstract = {Multiple Instance Learning (MIL) is a learning paradigm, where training instances are arranged in sets, called bags, and only bag-level labels are available during training. This learning paradigm has been successfully applied in various real-world scenarios, including medical image analysis, object detection, image classification, drug activity prediction, and many others. This survey paper presents a comprehensive analysis of MIL, highlighting its significance, recent advancements, methodologies, applications, and evolving trends across diverse domains. The survey begins by explaining the core principles that form the basis of MIL and how it differs from traditional learning approaches. This sets the foundation for comprehending the distinct challenges and techniques of solving MIL problems. Next, we discuss how supervised learning algorithms are tailored to support MIL and combine this discussion with a review of seminal MIL algorithms as well as the latest innovations that incorporate neural networks, deep learning architectures, and attention techniques. This comprehensive analysis helps to understand the strengths, limitations, and adaptability of these methods across diverse data modalities, complexities, and applications. In summary, this survey paper provides an essential resource for researchers, practitioners, and enthusiasts seeking a comprehensive understanding of Multiple Instance Learning. It covers foundational concepts, traditional methods, recent advancements, and future directions. By providing a holistic view of MIL’s dynamic landscape, this paper aims to inspire further innovation and exploration in this ever-evolving field.}
}
@incollection{2024II1,
title = {Index},
editor = {John R. Vacca},
booktitle = {Computer and Information Security Handbook (Fourth Edition)},
publisher = {Morgan Kaufmann},
edition = {Fourth Edition},
pages = {II1-II29},
year = {2024},
isbn = {978-0-443-13223-0},
doi = {https://doi.org/10.1016/B978-0-443-13223-0.20002-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780443132230200027}
}
@incollection{TODESCHINI2020599,
title = {4.25 - Chemometrics for QSAR Modeling☆},
editor = {Steven Brown and Romà Tauler and Beata Walczak},
booktitle = {Comprehensive Chemometrics (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {599-634},
year = {2020},
isbn = {978-0-444-64166-3},
doi = {https://doi.org/10.1016/B978-0-12-409547-2.14703-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780124095472147031},
author = {Roberto Todeschini and Viviana Consonni and Davide Ballabio and Francesca Grisoni},
keywords = {Applicability domain, Consensus modelling, Molecular descriptors, QSAR, QSPR, Ranking, Validation, Variable selection},
abstract = {Chemometrics plays a fundamental role in quantitative structure-activity relationships (QSARs) and quantitative structure-property relationships (QSPRs) methods, which aim at empirically linking the molecular structure of chemicals to experimentally-measurable properties. In fact, chemometrics statistics and chemoinformatics are the basic tools for finding meaningful mathematical relationships between the molecular structure and biological, physico-chemical, toxicological and environmental properties of chemicals. The key elements of QSAR/QSPR are molecular descriptors, which are numerical indices encoding information related to the structure of chemicals and are used as independent variables in the subsequent modeling, thus connecting the QSAR approaches to the multivariate and chemometric world. In this article, historical and novel QSAR modeling approaches are presented. After describing the historical background of QSAR and the classical approaches, molecular descriptors are illustrated, with state-of-the-art as well as novel description methodologies. Additionally, the key principles of QSAR are introduced, along with specific elements of the QSAR modeling workflow, e.g., variable reduction and selection, similarity-based approaches, validation, the definition of applicability domain and consensus modeling.}
}
@article{CANTEROGAMITO2023102673,
title = {The influence of China in AI governance through standardisation},
journal = {Telecommunications Policy},
volume = {47},
number = {10},
pages = {102673},
year = {2023},
issn = {0308-5961},
doi = {https://doi.org/10.1016/j.telpol.2023.102673},
url = {https://www.sciencedirect.com/science/article/pii/S0308596123001842},
author = {Marta {Cantero Gamito}},
abstract = {Artificial intelligence systems (AIS) are subject to technical standardisation. Technical standards are primarily developed within standard developing organisations (SDOs) traditionally operating under consensus-based, community- and largely industry-driven processes. Governments are increasingly interested in technical standards’ development, accentuating the political dimension of standardisation. This article explores the contribution of technical standardisation to the governance of artificial intelligence (AI) and asks whose views are being implemented in the development of non-state rules for AI. The article, based on empirical research, focuses on the changing governance structure of the International Telecommunications Union (ITU). Overall, the discussion offers an overview of the existing geopolitics in AI-related standardisation and contributes to the scholarship on AI and digital governance by exploring the role of technical standardisation as a tool in AI governance. The research finds an increasing Chinese representation in international standardisation and argues that the political use of standardisation can lead to China establishing its own vision of digital governance. Consequently, the article suggest that China is using participation in recognised SDOs to legitimate its vision for digital governance calling for a re-examination of standardisation considering its implications for democracy and the protection of human rights.}
}
@article{SIEVI2025105093,
title = {(How) Should security authorities counter false information on social media in crises? A democracy-theoretical and ethical reflection},
journal = {International Journal of Disaster Risk Reduction},
volume = {116},
pages = {105093},
year = {2025},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2024.105093},
url = {https://www.sciencedirect.com/science/article/pii/S2212420924008550},
author = {Luzia Sievi and Maria Pawelec},
keywords = {Disinformation, Fake news, Security authority, Crisis communication, Debunking, Disaster ethics},
abstract = {False information on social media accompanies most crises and exacerbates their complexity and consequences. If security authorities and organisations (SAO) want to ensure security in crises, they must therefore curb the spread of false information. However, in liberal democracies, state authorities, but also aid organisations taking on state tasks have a special responsibility to ensure that they do not unjustifiably impair public communication and citizens' constitutionally protected rights when combating false information. (How) Should SAO therefore react to false information? Which ethical questions and value conflicts arise? More concretely, how can SAO implement countermeasures without harming pluralist deliberation and violating democratic principles or values constitutive to their self-understanding? This paper assesses these questions, combining both descriptive and normative ethics and focusing on values such as liberty, autonomy, neutrality, privacy, non-discrimination, and security. It ethically evaluates four countermeasures for SAO to combat false information: Media literacy trainings, social media monitoring, preventive and reactive crisis communication, and community management. It also draws on various methods of qualitative social science research and evidence from the relevant scientific literature to uncover underlying values and value conflicts when it comes to SAO's reactions to false information on social media, and to contextualize the presented ethical considerations with regard to SAO's daily work and challenges. The paper contributes disaster, security, and media ethics and, more practically, to more effective and ethically informed strategies for democratic actors responding to disinformation and misinformation on social media.}
}
@article{CZECH20231867,
title = {The Lancet Commission on medicine, Nazism, and the Holocaust: historical evidence, implications for today, teaching for tomorrow},
journal = {The Lancet},
volume = {402},
number = {10415},
pages = {1867-1940},
year = {2023},
issn = {0140-6736},
doi = {https://doi.org/10.1016/S0140-6736(23)01845-7},
url = {https://www.sciencedirect.com/science/article/pii/S0140673623018457},
author = {Herwig Czech and Sabine Hildebrandt and Shmuel P Reis and Tessa Chelouche and Matthew Fox and Esteban González-López and Etienne Lepicard and Astrid Ley and Miriam Offer and Avi Ohry and Maike Rotzoll and Carola Sachse and Sari J Siegel and Michal Šimůnek and Amir Teicher and Kamila Uzarczyk and Anna {von Villiez} and Hedy S Wald and Matthew K Wynia and Volker Roelcke}
}
@article{PICCOLI2024101835,
title = {Digital transformation requires digital resource primacy: Clarification and future research directions},
journal = {The Journal of Strategic Information Systems},
volume = {33},
number = {2},
pages = {101835},
year = {2024},
issn = {0963-8687},
doi = {https://doi.org/10.1016/j.jsis.2024.101835},
url = {https://www.sciencedirect.com/science/article/pii/S0963868724000179},
author = {Gabriele Piccoli and Varun Grover and Joaquin Rodriguez},
keywords = {Digital transformation, IT-enabled transformation, Digital ontology, Digital organization, Digital resources},
abstract = {Responding to recent calls, this essay offers a commentary on the framing and definition of organizational digital transformation. We focus on the unique ontology of digital transformation and delineate it from neighboring concepts.Our contention is that, despite its volume, current research remains unclear about how the digital transformation of organizations differs from their IT-enabled transformation. We advocate definitional precision to foster knowledge accumulation and to enable scholars to pursue important research questions that are unique to digital transformation. Our perspective, grounded in the notion of digital resources, defines digital transformation as the metamorphosis of an IT-enabled organization into a digital organization – one with a specific digital architecture and design principles.A key departure from previous conceptualization is that we characterize digital transformation as a change in digital technology architecture rather than a change from digital technology use. Our paper achieves the following: describes the constructs underpinning this formulation, digital resources and digital organization; justifies their use; and describes what research directions the new perspective promotes. With sound definitions of key constructs, Information Systems scholars have the unprecedented opportunity to lead the way in digital “x” research, making our discipline the reference point for the burgeoning “digital research” literature in related business fields.}
}
@article{BAYS2023100065,
title = {Artificial intelligence and obesity management: An Obesity Medicine Association (OMA) Clinical Practice Statement (CPS) 2023},
journal = {Obesity Pillars},
volume = {6},
pages = {100065},
year = {2023},
issn = {2667-3681},
doi = {https://doi.org/10.1016/j.obpill.2023.100065},
url = {https://www.sciencedirect.com/science/article/pii/S2667368123000116},
author = {Harold Edward Bays and Angela Fitch and Suzanne Cuda and Sylvia Gonsahn-Bollie and Elario Rickey and Joan Hablutzel and Rachel Coy and Marisa Censani},
keywords = {Adiposopathy, Artificial intelligence, Education, Obesity},
abstract = {Background
This Obesity Medicine Association (OMA) Clinical Practice Statement (CPS) provides clinicians an overview of Artificial Intelligence, focused on the management of patients with obesity.
Methods
The perspectives of the authors were augmented by scientific support from published citations and integrated with information derived from search engines (i.e., Chrome by Google, Inc) and chatbots (i.e., Chat Generative Pretrained Transformer or Chat GPT).
Results
Artificial Intelligence (AI) is the technologic acquisition of knowledge and skill by a nonhuman device, that after being initially programmed, has varying degrees of operations autonomous from direct human control, and that performs adaptive output tasks based upon data input learnings. AI has applications regarding medical research, medical practice, and applications relevant to the management of patients with obesity. Chatbots may be useful to obesity medicine clinicians as a source of clinical/scientific information, helpful in writings and publications, as well as beneficial in drafting office or institutional Policies and Procedures and Standard Operating Procedures. AI may facilitate interactive programming related to analyses of body composition imaging, behavior coaching, personal nutritional intervention & physical activity recommendations, predictive modeling to identify patients at risk for obesity-related complications, and aid clinicians in precision medicine. AI can enhance educational programming, such as personalized learning, virtual reality, and intelligent tutoring systems. AI may help augment in-person office operations and telemedicine (e.g., scheduling and remote monitoring of patients). Finally, AI may help identify patterns in datasets related to a medical practice or institution that may be used to assess population health and value-based care delivery (i.e., analytics related to electronic health records).
Conclusions
AI is contributing to both an evolution and revolution in medical care, including the management of patients with obesity. Challenges of Artificial Intelligence include ethical and legal concerns (e.g., privacy and security), accuracy and reliability, and the potential perpetuation of pervasive systemic biases.}
}
@article{MA2025124125,
title = {Fighting fake news in the age of generative AI: Strategic insights from multi-stakeholder interactions},
journal = {Technological Forecasting and Social Change},
volume = {216},
pages = {124125},
year = {2025},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2025.124125},
url = {https://www.sciencedirect.com/science/article/pii/S0040162525001568},
author = {Rui Ma and Xueqing Wang and Guo-Rui Yang},
keywords = {Artificial intelligence (AI)-generated fake news, Fake news governance, Adaptive governance theory, User-generated content (UGC) platform, Opinion leader, Public opinion monitor agency (POMA)},
abstract = {The advancements in algorithm technology have led to a proliferation of artificial intelligence-generated fake news, resulting in significant social harm. Promoting multi-stakeholder engagement in fake news governance is beneficial for establishing a robust information ecosystem. The primary stakeholders, including the government at the policy-making end, user-generated content platforms at the algorithm development end, and opinion leaders at the news dissemination end, possess varying degrees of initiative and roles in governance. The main objective of this study is to investigate the evolutionary process of behaviors among multi-stakeholders in fake news governance and their influencing factors under different news environments. This study constructs an evolutionary game model to identify the conditions for the realization of five models of fake news governance. Stakeholders' behaviors in different states are affected by external factors, such as news environment, penalties, and incentives, as well as internal factors, such as governance capability deficiencies and platform algorithm reliability. The research findings expand the boundary of adaptive governance theory by revealing the mechanisms of stakeholder collaboration and the interaction between stakeholders and the news environment in fake news governance. These insights offer valuable guidance for advancing the transformation and enhancement of fake news governance models.}
}
@article{ASHIK2025115053,
title = {Can marketing reduce inequality? Evidence from marketing science},
journal = {Journal of Business Research},
volume = {188},
pages = {115053},
year = {2025},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2024.115053},
url = {https://www.sciencedirect.com/science/article/pii/S0148296324005575},
author = {Farhan Ashik and Weng {Marc Lim} and Jarrod P. Vassallo and Ranjit Voola},
keywords = {Inequality, Marketing, Systematic Literature Review, Sustainable Development Goals},
abstract = {Reducing inequality is integral to ensuring that no one is left behind. As a discipline, marketing can play a significant role in addressing inequality. To understand the current status and potential of marketing theory and practice to reduce inequality, a comprehensive and systematic literature review is needed. Addressing this gap, we conduct a systematic review of 313 marketing studies on inequalities. Our review highlights that inequality manifests in numerous ways—including consumption, culture, digital, economic, education, gender, geographical, health, income, power, racial, social, socioeconomic, structural, wealth, and general inequalities—and identifies five broad categories of antecedents and outcomes (individual, family, environmental, organization, and country). However, many studies lack specificity in defining and examining these inequalities, hindering the development of targeted interventions. To address this, we propose new definitions for each type of inequality and outline clear pathways for future research. These contributions not only highlight current progress in the field but also establish a roadmap for advancing marketing scholarship on inequality.}
}
@article{ZIVIC2025113525,
title = {Materials informatics: A review of AI and machine learning tools, platforms, data repositories, and applications to architectured porous materials},
journal = {Materials Today Communications},
volume = {48},
pages = {113525},
year = {2025},
issn = {2352-4928},
doi = {https://doi.org/10.1016/j.mtcomm.2025.113525},
url = {https://www.sciencedirect.com/science/article/pii/S2352492825020379},
author = {Fatima Zivic and Ana Kaplarevic Malisic and Nenad Grujovic and Boban Stojanovic and Milos Ivanovic},
keywords = {Traditional computational models, Data-driven AI material models, Smart materials, Deep Tech, Structure-property-processing relationships, High-throughput screening, Electrospinning, 3D printed biomimetic porosity},
abstract = {This review presents the key aspects and development directions of materials informatics, emphasizing the role of artificial intelligence (AI) and machine learning (ML) in materials science research. The objective is to provide a comprehensive overview of materials informatics tools, workflows, and case studies, particularly aimed at experimental researchers unfamiliar with AI frameworks. Basic concepts are introduced and traditional modelling methods compared to AI/ML-assisted models. Existing material models serve as a foundation for advanced modelling and simulations aimed at reducing the time required for characterisation and discovery, with physics-based models gaining importance in the development of AI-supported surrogate models. This review also covers currently available resources, including: (i) software for solving complex mathematical equations and material modelling; (ii) web-based platforms and tools designed for both expert and non-expert users; and (iii) materials data repositories, prioritising standardisation. Case examples involving materials with architectured macro-, micro-, and nano-porosity are reviewed across three material types: metal-organic frameworks (MOFs), electrospun PVDF piezoelectrics, and 3D printed mechanical metamaterials. Traditional computational models offer interpretability and physical consistency, AI/ML excels in speed and complexity handling but may lack transparency. Hybrid models combining both approaches show excellent results in prediction, simulation, and optimisation, offering both speed and interpretability. Progress depends on modular, interoperable AI systems, standardised FAIR data, and cross-disciplinary collaboration. Addressing data quality and integration challenges will resolve issues related to metadata gaps, semantic ontologies, and data infrastructures, especially for small datasets and unlock transformative advances in fields like nanocomposites, MOFs, and adaptive materials.}
}
@article{YU2026103113,
title = {Transformation of industrial robotics with natural language models: Recent progress and future prospects},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {97},
pages = {103113},
year = {2026},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2025.103113},
url = {https://www.sciencedirect.com/science/article/pii/S073658452500167X},
author = {Zhao Yu and Peize Zhang and Jing Shi},
keywords = {Natural language models, Industrial robots, Industry 4.0/5.0, Human-robot interactions, Regulatory considerations},
abstract = {Integration of Natural Language Models (NLMs) into industrial robots enhances operational efficiency and intuitive human-robot interactions, and thus it represents a significant opportunity in the pursuit of Industry 4.0/5.0. This paper provides a comprehensive survey on the technological advancements and applications in this area, by emphasizing their role in improving task execution, cognitive capabilities, and communication in the industrial environments. Meanwhile, related challenges are analyzed and discussed. In particular, NLMs inherently struggle with contextual understanding, which can lead to inappropriate or impractical outputs in complex industrial environments. Also, the external noise and the need for real-time responsiveness present further complications to the effectiveness of NLMs. Concerns regarding safety, transparency, privacy, and ethical usage amplify the need for regulatory considerations. In addition, standardized approaches to interpreting vague human instructions are called for to improve the interaction between humans and robots. It is pointed out that the broader impacts of NLMs can extend beyond industrial environments into commercial and social settings, thereby enhancing service quality and customer interactions. As a result, the review is expected to provide insights on how to effectively integrate NLMs with robotic systems, stimulate research to address the remaining challenges, and enhance transparency to improve social acceptability.}
}
@article{WEIMANN2025222,
title = {ESPEN guideline on clinical nutrition in surgery – Update 2025},
journal = {Clinical Nutrition},
volume = {53},
pages = {222-261},
year = {2025},
issn = {0261-5614},
doi = {https://doi.org/10.1016/j.clnu.2025.08.029},
url = {https://www.sciencedirect.com/science/article/pii/S0261561425002432},
author = {Arved Weimann and Mihailo Bezmarevic and Marco Braga and M. Isabel T.D. Correia and Pamela Funk-Debleds and Luca Gianotti and Chelsia Gillis and Martin Hübner and Jesus Fernando B. Inciong and Mohammad Shukri Jahit and Stanislaw Klek and Takayuki Kori and Alessandro Laviano and Olle Ljungqvist and Dileep N. Lobo and Carmelo Loinaz Segurola and Isacco Montroni and B. Ravinder Reddy and Nicole M. Saur and Anna Schweinlin and Han-Ping Shi and Hiroya Takeuchi and Dan L. Waitzberg and Ola Wallengren and Paul E. Wischmeyer and Dirk Ysebaert and Stephan C. Bischoff},
keywords = {Enhanced recovery after surgery, Enteral nutrition, Parenteral nutrition, Perioperative nutrition, Prehabilitation, Surgery},
abstract = {Summary
Early oral feeding is the preferred mode of nutrition for surgical patients. Avoidance of any nutritional therapy bears the risk of underfeeding during the postoperative course after major surgery. Considering that malnutrition and underfeeding are risk factors for postoperative complications, nutritional therapy is mandatory for any surgical patient at nutritional risk, especially for those undergoing upper gastrointestinal surgery. The focus of this guideline is to cover nutritional aspects of the Enhanced Recovery After Surgery (ERAS) concept and the special nutritional needs of patients undergoing major surgery, e.g. for cancer, and of those developing severe complications despite best perioperative care. From a metabolic and nutritional point of view, the key aspects of perioperative care include: a) Integration of nutrition into the overall management of the patient, b) avoidance of long periods of preoperative fasting c) re-establishment of oral feeding as early as possible after surgery d) start of nutritional therapy early, as soon as a nutritional risk becomes apparent e) metabolic control e.g. of blood glucose, f) reduction of factors which exacerbate stress-related catabolism or impair gastrointestinal function, g) minimized time on paralytic agents in the postoperative period, and h) early mobilization to facilitate protein synthesis and muscle function. The guideline presents 44 recommendations for clinical practice in patients undergoing elective and non-elective surgery, including new recommendations for frailty assessment, sarcopenia diagnosis, and prehabilitation. As in the former ESPEN practical guideline, the recommendations were additonally presented in decision-making flowcharts.}
}
@article{BENNEHALLI2025101172,
title = {A review on the formation, recovery, and properties of coal fly ash (CFA)-derived microspheres for sustainable technologies and biomedical applications},
journal = {Next Materials},
volume = {9},
pages = {101172},
year = {2025},
issn = {2949-8228},
doi = {https://doi.org/10.1016/j.nxmate.2025.101172},
url = {https://www.sciencedirect.com/science/article/pii/S2949822825006902},
author = {Basavaraju Bennehalli and Suresh Subramanyam Poyil and Budigi Lokesh and Santhosh Nagaraja and Sunil Basavaraju and  Rispandi and Muhammad Imam Ammarullah},
keywords = {Coal fly ash (CFA), Cenospheres (CS), Plerospheres (PS), Ferrospheres (FS), Microspheres},
abstract = {Coal fly ash (CFA), a by-product of coal combustion in thermal power plant (TPP), is an environmental concern due to its massive production and improper disposal. Among its components, microspheres like cenospheres (CS), plerospheres (PS), and ferrospheres (FS) hold significant industrial value. CS are lightweight, hollow particles with unique properties such as low density, high mechanical strength, and thermal stability, making them suitable for composites, ceramics, and insulation. PS, with their porous structures, are useful in construction and ceramics, while FS, rich in iron, are applied in catalysis and magnetic materials. Additionally, CFA-derived microspheres, such as CS and FS, exhibit promising potential in biomedical applications due to their unique structural and chemical features. Their suitability for drug delivery, tissue engineering, and diagnostic tools highlights their emerging role in sustainable healthcare solutions. This review focuses on the formation, recovery, and properties of these microspheres, highlighting their sustainable applications in lightweight composites, environmental clean-up, and advanced materials. Various recovery methods, including wet and dry techniques, are discussed to optimize extraction processes. The study emphasizes the potential of these microspheres in reducing CFA waste while supporting innovative and eco-friendly technologies. This work contributes to developing sustainable solutions for managing CFA, with the goal of reducing environmental impacts and enhancing industrial utility, particularly in sustainable and biomedical applications.}
}
@incollection{CHOU20241,
title = {Chapter I - A new alternative concept for cost-effective R&D: The MAL-dynamics/algorithms/digital informatics⊛⊛This book has a companion website hosting complementary materials. Visit this URL to access it: https://www.elsevier.com/books-and-journals/book-companion/9780443288746.},
editor = {Ting-Chao Chou},
booktitle = {Mass-Action Law Dynamics Theory and Algorithm for Translational and Precision Medicine Informatics},
publisher = {Academic Press},
pages = {1-37},
year = {2024},
isbn = {978-0-443-28874-6},
doi = {https://doi.org/10.1016/B978-0-443-28874-6.00001-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780443288746000019},
author = {Ting-Chao Chou},
keywords = {Mass-action law dynamics, Bioinformatics algorithms, Biodynamics simulation, Pharmacodynamics, Median-effect equation, Median-effect plot and simulation, Doctrine of the median, Combination index equation, Combination index computer simulation, CompuSyn software, CalcuSyn software, Dose-reduction index equation, Dose-reduction index computer simulation, Definition of synergism, Definition of pharmacodynamics},
abstract = {This chapter provides a comprehensive overview, illustrations, and updates of the mass-action law (MAL) based on unified general dose-effect pharmacodynamics, biodynamics, bioinformatics, and the combination index theorem for synergy definition and quantification (MAL-PD/BD/BI/CI). The general system analysis was developed with pattern analysis, combinatory analysis, and mathematical induction and deduction on the MAL principle, which resulted in (i) The general median-effect equation (MEE) for each entity with two basic dynamic parameters of potency (Dm) and dynamics-order (m), for the potency and the shape of dose-effect curves; (ii) the combination index equation (CIE) theorem/algorithm, determines the drugs or entities interaction, where CI < 1, =1, and >1 indicate synergism, additive-effect, and antagonism, respectively; (iii) the dose-reduction index equation (DRIE) that digitally determines the outcomes of how many folds of dose-reduction for each drug in synergistic combinations. Since all terms of MAL-general-equations (MEE, CIE, and DRIE) are dimensionless-relativity ratios, they are equally applicable in vitro, in animals, and in clinical trials. They are also valid regardless of drug entities, units, mechanisms, or physical states. To date, this MAL-PD/BD/BI/CI theory-based “Top-Down” approach and quantitative method has received multidisciplinary research popularity globally, with citations in over 1500 journals and 1268 citing patents, encompassing nearly all disciplines of biomedical sciences and beyond, the subjects including material, agricultural, marine, environmental and food sciences. Original applications were mainly in vitro; however, in vivo PD applications are on the rise. The MAL-BD/PD unified theory/method has the features of simplicity, efficiency, and cost-effectiveness, allows automated computer simulation with only a few dose-data points, and shares the same basic MAL principle. In vitro, in vivo, or other physical states in animal studies and clinical trial protocol design, automated computerized data analysis and simulation to single drug and drug combinations. With the same MAL-PD principle, thus enabling comparisons and rankings. This book illustrates the MAL-dynamics theory to update the increasing applications in recent years and to provide specific real sample analysis, including data entries and automated computer report print-outs in Appendixes and Supplementary Materials in PDF slides illustrations. The MAL-theory-based Top Down approach (traditional biomedical R&D) is an observation and statistics-based “Bottom-Up” approach with specific aims, proposals, and methods to reach feasible hypotheses or conclusions. This open approach is usually accomplished with multiple experimental evidence and results, using unbiased statistics or other methods to reach a hypothesis, mechanism, or interpretive conclusion. However, the best curve fitting for dose-effect relationship data is frequently empirical and requires many dose-data points. The primary purpose of this book is to indicate that the MAL theory/algorithm-based “Top-Down” digital approach is the opposite and yet a complementary alternative to the observation/statistics-based “Bottom-Up” traditional approach in R&D.}
}
@article{AMMARULLAH2025110518,
title = {A review of enhanced total hip prosthesis design and material bearing combination to accommodate Muslim prayer (Salat) movements: Biomechanical, biotribological, and biological perspectives},
journal = {Tribology International},
volume = {205},
pages = {110518},
year = {2025},
issn = {0301-679X},
doi = {https://doi.org/10.1016/j.triboint.2025.110518},
url = {https://www.sciencedirect.com/science/article/pii/S0301679X25000131},
author = {Muhammad Imam Ammarullah and Muhammad Kozin and Mohamad Izzur Maula and M. {Danny Pratama Lamura} and Hasyid Ahmad Wicaksono and Athanasius Priharyoto Bayuseno and Jamari Jamari and Muhammad Hanif Ramlee},
keywords = {Total hip prosthesis, Design, Bearing, Muslim prayer (Salat)},
abstract = {Total hip prostheses have greatly improved mobility and quality of life for patients with hip disorders. However, the unique biomechanical demands of Muslim prayer (Salat), involving complex, repetitive movements, pose challenges for standard designs. This review highlights advancements in prosthesis design, emphasizing dual mobility bearings for enhanced stability and range of motion, and Ceramic-on-Polymer (CoP) bearings for durability and reduced wear. By addressing biomechanical, biotribological, and biological factors, these innovations optimize prosthetic performance, meeting the functional and cultural needs of Muslim patients while ensuring long-term durability and satisfaction.}
}
@article{FERRAG20251,
title = {Generative AI in cybersecurity: A comprehensive review of LLM applications and vulnerabilities},
journal = {Internet of Things and Cyber-Physical Systems},
volume = {5},
pages = {1-46},
year = {2025},
issn = {2667-3452},
doi = {https://doi.org/10.1016/j.iotcps.2025.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S2667345225000082},
author = {Mohamed Amine Ferrag and Fatima Alwahedi and Ammar Battah and Bilel Cherif and Abdechakour Mechri and Norbert Tihanyi and Tamas Bisztray and Merouane Debbah},
keywords = {Generative AI, LLM, Transformer security, Cybersecurity},
abstract = {This paper provides a comprehensive review of the future of cybersecurity through Generative AI and Large Language Models (LLMs). We explore LLM applications across various domains, including hardware design security, intrusion detection, software engineering, design verification, cyber threat intelligence, malware detection, and phishing detection. We present an overview of LLM evolution and its current state, focusing on advancements in models such as GPT-4, GPT-3.5, Mixtral-8x7B, BERT, Falcon2, and LLaMA. Our analysis extends to LLM vulnerabilities, such as prompt injection, insecure output handling, data poisoning, DDoS attacks, and adversarial instructions. We delve into mitigation strategies to protect these models, providing a comprehensive look at potential attack scenarios and prevention techniques. Furthermore, we evaluate the performance of 42 LLM models in cybersecurity knowledge and hardware security, highlighting their strengths and weaknesses. We thoroughly evaluate cybersecurity datasets for LLM training and testing, covering the lifecycle from data creation to usage and identifying gaps for future research. In addition, we review new strategies for leveraging LLMs, including techniques like Half-Quadratic Quantization (HQQ), Reinforcement Learning with Human Feedback (RLHF), Direct Preference Optimization (DPO), Quantized Low-Rank Adapters (QLoRA), and Retrieval-Augmented Generation (RAG). These insights aim to enhance real-time cybersecurity defenses and improve the sophistication of LLM applications in threat detection and response. Our paper provides a foundational understanding and strategic direction for integrating LLMs into future cybersecurity frameworks, emphasizing innovation and robust model deployment to safeguard against evolving cyber threats.}
}
@article{ALKFAIRY2024100459,
title = {Factors impacting users’ willingness to adopt and utilize the metaverse in education: A systematic review},
journal = {Computers in Human Behavior Reports},
volume = {15},
pages = {100459},
year = {2024},
issn = {2451-9588},
doi = {https://doi.org/10.1016/j.chbr.2024.100459},
url = {https://www.sciencedirect.com/science/article/pii/S2451958824000927},
author = {Mousa Al-kfairy and Soha Ahmed and Ashraf Khalil},
keywords = {Metaverse education, Intention to use, Acceptance, Systematic review, Information systems theories},
abstract = {Purpose
This study explores the factors influencing the adoption and acceptance of Metaverse technologies in educational settings. Despite the growing interest in immersive educational environments provided by the Metaverse, there is a lack of comprehensive understanding regarding the elements that affect user engagement and acceptance. This paper aims to bridge this gap through a systematic review of empirical studies that apply Information Systems theories such as TAM, UTAUT, TPB, and their extensions.
Methods
A total of 35 empirical studies were analyzed using a methodical review approach. The research methodologies employed in these studies include surveys, structural equation modeling, and interviews, providing a broad spectrum of data on how different factors influence educational outcomes in the Metaverse.
Results
The findings reveal that user adoption of the Metaverse in educational contexts is influenced by multiple factors at individual, technological, and environmental levels. Key factors identified include effort expectancy, behavioral intention, self-efficacy, enjoyment, and immersion. These factors are subject to moderating effects, suggesting that the dynamics of Metaverse adoption are highly context-dependent.
Conclusion
The insights gained from this review provide valuable guidelines for educators, policymakers, and technology developers aiming to effectively integrate Metaverse technologies into educational frameworks. The study also outlines limitations and suggests directions for future research, highlighting the need for further investigations into the longitudinal impacts and cultural adaptability of Metaverse applications in education.}
}
@article{BECK2024100524,
title = {Boundary work and high-reliability organizing in interorganizational collaborations},
journal = {Information and Organization},
volume = {34},
number = {3},
pages = {100524},
year = {2024},
issn = {1471-7727},
doi = {https://doi.org/10.1016/j.infoandorg.2024.100524},
url = {https://www.sciencedirect.com/science/article/pii/S1471772724000241},
author = {Tammy E. Beck and Stephanie T. Solansky and Daniel J. Davis and Karen Ford-Eickhoff and Donde Plowman},
keywords = {High reliability organizing, HRO collaboration, Boundary work, Boundary object},
abstract = {Consider the massive recovery response that included over 25,000 professionals and volunteers representing more than 120 organizations tasked with locating both human remains and vehicle debris following the Columbia Space Shuttle tragedy. Despite the daunting scope of the initial search area – 2.28 million acres of land – participating members were successful in their efforts to achieve the collective's goals. We contend that the response effort was effective because relatively disparate organizations and governmental agencies came together and ultimately exemplified the hallmarks of high reliability organizing (HRO). Our study explores how the transition in boundaries made this possible. Using interview and secondary data from our case study, we explore how individuals engaged in boundary work that facilitated boundary transformation. Specifically, we document how individuals interacted with a data visualization system to temper the physical, social, temporal, and scope boundary tensions initially present following the disaster. Amidst an emergent, messy, and complex setting, the interaction with a boundary object allowed for unity in diversity of participating organizations, a common language through mapping, a form of trichordal temporal and rapid sensemaking, and a foundation for dynamic decision making. Therefore, our study yields critical insights into how organizational members engage in boundary work to aid HRO collaborations.}
}
@article{SANCHEZWELLS2025126757,
title = {Truck-multidrone same-day delivery strategies: On-road resupply vs depot return},
journal = {Expert Systems with Applications},
volume = {272},
pages = {126757},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.126757},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425003793},
author = {David Sanchez-Wells and José L. Andrade-Pineda and Pedro L. Gonzalez-R},
keywords = {Truck-Multidrone Logistics, Genetic Algorithm, Makespan, Truck Mileage, Last-mile Delivery, Resupply},
abstract = {This paper explores an enhanced two-waved same-day delivery (SDD) system that leverages a mothership truck equipped with multiple drones supported by an auxiliary “resupply” truck. Under standard SDD operations, this mothership truck, also capable of performing deliveries, must return to the depot to reload, incurring extra travel time and mileage. In contrast, the proposed resupply strategy enables the second delivery wave by dispatching a secondary vehicle to meet the mothership truck on-road, reloading parcels without interrupting ongoing deliveries by the drones. A single unified routing framework, the Genetic Algorithm with Iterated Estimations for Resupply (GAIER), is presented to optimise both strategies under two selectable criteria: minimising total service time or total truck mileage. In tests with benchmark networks of different sizes (20, 50, and 75 nodes), incorporating a resupply truck reduced every selected criterion when compared to the strategy where the mothership vehicle returns to the depot. Subsequent comparative analysis points an average reduction of 17 % in service time and 21 % in truck mileage while statistical analyses support the strategy choice significancy, confirming resupply strategy’s potential for cost savings and reduced environmental impact. These findings bolster our proposition that incorporating a resupply truck into hybrid truck-multidrone systems enhances flexibility in drone delivery scheduling and improves the system’s ability to meet urban demand.}
}
@article{DENG2025105224,
title = {Does ChatGPT enhance student learning? A systematic review and meta-analysis of experimental studies},
journal = {Computers & Education},
volume = {227},
pages = {105224},
year = {2025},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2024.105224},
url = {https://www.sciencedirect.com/science/article/pii/S0360131524002380},
author = {Ruiqi Deng and Maoli Jiang and Xinlu Yu and Yuyan Lu and Shasha Liu},
keywords = {Teaching/learning strategies, Improve classroom teaching, Elementary education, Secondary education, Post-secondary education},
abstract = {Chat Generative Pre-Trained Transformer (ChatGPT) has generated excitement and concern in education. While cross-sectional studies have highlighted correlations between ChatGPT use and learning performance, they fall short of establishing causality. This review examines experimental studies on ChatGPT's impact on student learning to address this gap. A comprehensive search across five databases identified 69 articles published between 2022 and 2024 for analysis. The findings reveal that ChatGPT interventions are predominantly implemented at the university level, cover various subject areas focusing on language education, are integrated into classroom environments as part of regular educational practices, and primarily involve direct student use of ChatGPT. Overall, ChatGPT improves academic performance, affective-motivational states, and higher-order thinking propensities; it reduces mental effort and has no significant effect on self-efficacy. However, methodological limitations, such as the lack of power analysis and concerns regarding post-intervention assessments, warrant cautious interpretation of results. This review presents four propositions from the findings: (1) distinguish between the quality of ChatGPT outputs and the positive effects of interventions on academic performance by shifting from well-defined problems in post-intervention assessments to more complex, project-based assessments that require skill demonstration, adopting proctored assessments, or incorporating metrics such as originality alongside quality; (2) evaluate long-term impacts to determine whether the positive effects on affective-motivational states are sustained or merely owing to novelty effect; (3) prioritise objective measures to complement subjective assessments of higher-order thinking; and (4) use power analysis to determine adequate sample sizes to avoid Type II errors and provide reliable effect size estimates. This review provides valuable insights for researchers, instructors, and policymakers evaluating the effectiveness of generative AI integration in educational practice.}
}
@article{BUJA2024104944,
title = {Pathobiology of myocardial and cardiomyocyte injury in ischemic heart disease: Perspective from seventy years of cell injury research},
journal = {Experimental and Molecular Pathology},
volume = {140},
pages = {104944},
year = {2024},
issn = {0014-4800},
doi = {https://doi.org/10.1016/j.yexmp.2024.104944},
url = {https://www.sciencedirect.com/science/article/pii/S0014480024000649},
author = {L. Maximilian Buja},
keywords = {Cardiomyocyte, Myocardial ischemia, Calcium measurements, Pathology, Reperfusion, Conditioning, Oncosis, Apoptosis, Autophagy},
abstract = {This review presents a perspective on the pathobiology of acute myocardial infarction, a major manifestation of ischemic heart disease, and related mechanisms of ischemic and toxic cardiomyocyte injury, based on advances and insights that have accrued over the last seventy years, including my sixty years of involvement in the field as a physician-scientist-pathologist. This analysis is based on integration of my research within the broader context of research in the field. A particular focus has been on direct measurements in cardiomyocytes of electrolyte content by electron probe X-ray microanalysis (EPXMA) and Ca2+ fluxes by fura-2 microspectrofluorometry. These studies established that increased intracellular Ca2+ develops at a transitional stage in the progression of cardiomyocyte injury in association with ATP depletion, other electrolyte alterations, altered cell volume regulation, and altered membrane phospholipid composition. Subsequent increase in total calcium with mitochondrial calcium accumulation can occur. These alterations are characteristic of oncosis, which is an initial pre-lethal state of cell injury with cell swelling due to cell membrane dysfunction in ATP depleted cells; oncosis rapidly progresses to necrosis/necroptosis with physical disruption of the cell membrane, unless the adverse stimulus is rapidly reversed. The observed sequential changes fit a three-stage model of membrane injury leading to irreversible cell injury. The data establish oncosis as the primary mode of cardiomyocyte injury in evolving myocardial infarcts. Oncosis also has been documented to be the typical form of non-ischemic cell injury due to toxins. Cardiomyocytes with less energy impairment have the capability of undergoing apoptosis and autophagic death as well as oncosis, as is seen in pathological remodeling in chronic heart failure. Work is ongoing to apply the insights from experimental studies to better understand and ameliorate myocardial ischemia and reperfusion injury in patients. The perspective and insights in this review are derived from basic principles of pathology, an integrative discipline focused on mechanisms of disease affecting the cell, the organizing unit of living organisms.}
}
@article{CHEN2025125322,
title = {A novel air combat target threat assessment method based on three-way decision and game theory under multi-criteria decision-making environment},
journal = {Expert Systems with Applications},
volume = {259},
pages = {125322},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.125322},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424021894},
author = {Qihong Chen and Qingsong Zhao and Zhigang Zou and Qunyou Qian and Junuo Zhou and Renpeng Yuan},
keywords = {Threat assessment, Air combat target, Three-way decision, Multi-criteria decision-making, Game theory},
abstract = {Threat assessment of air combat target (TAoACT) is an important issue in air combat decision-making. Current methodologies for threat assessment are insufficient in capturing the systematic and time-sensitive characteristics of air combat, lacking a comprehensive analysis for the complex threat of air combat target (ACT). To address this issue, we propose a multi-criteria three-way decision-making (MCTWD) method based on game theory for TAoACT. The proposed method comprises three key components: (1) a single-criteria three-way decision (TWD) model based on the relative utility function with a new multi-criteria attribute system; (2) a multi-criteria aggregation mechanism based on game theory to analyze and integrate diverse decision outcomes derived from various criteria; and (3) decision conditions based on joint criteria to rank and classify ACTs. The effectiveness and applicability of the proposed method have been validated through three numerical examples. Comparative analysis results indicate that the proposed method demonstrates superior performance in ranking and classification when compared to five widely-used MCTWD methods.}
}